[transforms.tokenizer]
allow_you_to_description = """\
tokenize a field's value by splitting on white space, ignoring special \
wrapping characters, and zip the tokens into ordered field names\
"""
function_categories = ["parse"]
input_types = ["log"]
output_types = ["log"]

[transforms.tokenizer.options.field]
type = "string"
default = "message"
null = false
description = "The log field to tokenize."

[transforms.tokenizer.options.field_names]
type = "[string]"
examples = [["timestamp", "level", "message"]]
null = false
description = "The log field names assigned to the resulting tokens, in order."

[transforms.tokenizer.options.drop_field]
type = "bool"
default = true
null = false
description = "If `true` the `field` will be dropped after parsing."

[transforms.tokenizer.options.types]
type = "table"
null = true
description = "Key/Value pairs representing mapped log field types."

[transforms.tokenizer.options.types.options."*"]
type = "string"
enum = ["string", "int", "float", "bool", "timestamp|strftime"]
examples = [
  {name = "status", value = "int"},
  {name = "duration", value = "float"},
  {name = "success", value = "bool"},
  {name = "timestamp", value = "timestamp|%s", comment = "unix"},
  {name = "timestamp", value = "timestamp|%+", comment = "iso8601 (date and time)"},
  {name = "timestamp", value = "timestamp|%F", comment = "iso8601 (date)"},
  {name = "timestamp", value = "timestamp|%a %b %e %T %Y", comment = "custom strftime format"},
]
null = false
description = """\
A definition of mapped log field types. They key is the log field name and \
the value is the type. [`strftime` specifiers][urls.strftime_specifiers] are \
supported for the `timestamp` type.\
"""