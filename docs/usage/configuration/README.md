---
description: Vector configuration
---

<!---
!!!WARNING!!!!

This file is autogenerated! Please do not manually edit this file.
Instead, please modify the contents of `scripts/config_schema.toml`.
-->


# Configuration

![](../../assets/configure.svg)

This section covers configuring Vector and creating [pipelines](../../about/concepts.md#pipelines) like the one shown above. Vector requires only a _single_ [TOML](https://github.com/toml-lang/toml) configurable file, which you can specify via the [`--config` flag](../administration/starting.md#options) when [starting](../administration/starting.md) vector:

```bash
vector --config /etc/vector/vector.toml
```

## Example

{% code-tabs %}
{% code-tabs-item title="vector.toml" %}
```coffeescript
data_dir = "/var/lib/vector"

# Ingest data by tailing one or more files
[sources.apache_logs]
    type         = "file"
    include      = ["/var/log/apache2/*.log"]
    ignore_older = 86400 # 1 day

# Structure and parse the data
[transforms.apache_parser]
    inputs        = ["apache_logs"]
  type            = "regex_parser"
  regex           = '^(?P<host>[w.]+) - (?P<user>[w]+) (?P<bytes_in>[d]+) [(?P<timestamp>.*)] "(?P<method>[w]+) (?P<path>.*)" (?P<status>[d]+) (?P<bytes_out>[d]+)$'

# Sample the data to save on cost
[transforms.apache_sampler]
    inputs       = ["apache_parser"]
    type         = "sampler"
    hash_field   = "request_id" # sample _entire_ requests
    rate         = 10 # only keep 10%

# Send structured data to a short-term storage
[sinks.es_cluster]
    inputs       = ["apache_sampler"]
    type         = "elasticsearch"
    host         = "79.12.221.222:9200"
    doc_type     = "_doc"

# Send structured data to a cost-effective long-term storage
[sinks.s3_archives]
    inputs       = ["apache_parser"] # don't sample
    type         = "aws_s3"
    region       = "us-east-1"
    bucket       = "my_log_archives"
    batch_size   = 10000000 # 10mb uncompressed
    gzip         = true
    encoding     = "ndjson"
```
{% endcode-tabs-item %}
{% endcode-tabs %}

## Global Options

| Key  | Type  | Description |
| :--- | :---: | :---------- |
| `data_dir` | `string` | The directory used for persisting Vector state, such as on-disk buffers. Please make sure the Vector project has write permissions to this dir. See [Data Directory](#data-directory) for more info.<br />`no default` `example: "/var/lib/vector"` |

## Sources

| Name  | Description |
| :---  | :---------- |
| [**`file`**](sources/file.md) | Ingests data through one or more local files and outputs ["[`log`][log_event]"] events.<br />`guarantee: best_effort` |
| [**`statsd`**](sources/statsd.md) | Ingests data through the StatsD UDP protocol and outputs ["[`log`][log_event]"] events.<br />`guarantee: best_effort` |
| [**`stdin`**](sources/stdin.md) | Ingests data through standard input (STDIN) and outputs ["[`log`][log_event]"] events.<br />`guarantee: at_least_once` |
| [**`syslog`**](sources/syslog.md) | Ingests data through the Syslog 5424 protocol and outputs ["[`log`][log_event]"] events.<br />`guarantee: best_effort` |
| [**`tcp`**](sources/tcp.md) | Ingests data through the TCP protocol and outputs ["[`log`][log_event]"] events.<br />`guarantee: best_effort` |
| [**`vector`**](sources/vector.md) | Ingests data through another upstream Vector instance and outputs ["[`log`][log_event]"] events.<br />`guarantee: best_effort` |

[+ request a new source](https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature%2C%7B%3Atitle%3D%3E%22New+%60%3Cname%3E%60+source%22%7D&title=New+%60%3Cname%3E%60+source)

## Transforms

| Name  | Description |
| :---  | :---------- |
| [**`add_fields`**](transforms/add_fields.md) | Accepts ["[`log`][log_event]"] events and allows you to add one or more fields. |
| [**`field_filter`**](transforms/field_filter.md) | Accepts ["[`log`][log_event]", "[`metric`][metric_event]"] events and allows you to filter events by a field's value. |
| [**`grok_parser`**](transforms/grok_parser.md) | Accepts ["[`log`][log_event]"] events and allows you to parse a field value with [Grok][grok]. |
| [**`json_parser`**](transforms/json_parser.md) | Accepts ["[`log`][log_event]"] events and allows you to parse a field value as JSON. |
| [**`lua`**](transforms/lua.md) | Accepts ["[`log`][log_event]"] events and allows you to transform events with a full embedded [Lua][lua] engine. |
| [**`regex_parser`**](transforms/regex_parser.md) | Accepts ["[`log`][log_event]"] events and allows you to parse a field's value with a [Regular Expression][regex]. |
| [**`remove_fields`**](transforms/remove_fields.md) | Accepts ["[`log`][log_event]", "[`metric`][metric_event]"] events and allows you to remove one or more event fields. |
| [**`sampler`**](transforms/sampler.md) | Accepts ["[`log`][log_event]"] events and allows you to sample events with a configurable rate. |
| [**`tokenizer`**](transforms/tokenizer.md) | Accepts ["[`log`][log_event]"] events and allows you to tokenize a field's value by splitting on white space, ignoring special wrapping characters, and zipping the tokens into ordered field names. |

[+ request a new transform](https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature%2C%7B%3Atitle%3D%3E%22New+%60%3Cname%3E%60+transform%22%7D&title=New+%60%3Cname%3E%60+transform)

## Sinks

| Name  | Description |
| :---  | :---------- |
| [**`aws_cloudwatch_logs`**](sinks/aws_cloudwatch_logs.md) | Batches and flushes ["[`log`][log_event]"] events to [AWS CloudWatch Logs][aws_cw_logs] via the [`PutLogEvents` API endpoint](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutLogEvents.html).<br />`guarantee: at_least_once` |
| [**`aws_kinesis_streams`**](sinks/aws_kinesis_streams.md) | Batches and flushes ["[`log`][log_event]"] events to [AWS Kinesis Data Stream][aws_kinesis_data_streams] via the [`PutRecords` API endpoint](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html).<br />`guarantee: at_least_once` |
| [**`aws_s3`**](sinks/aws_s3.md) | Batches and flushes ["[`log`][log_event]"] events to [AWS S3][aws_s3] via the [`PutObject` API endpoint](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html).<br />`guarantee: at_least_once` |
| [**`blackhole`**](sinks/blackhole.md) | Streams ["[`log`][log_event]", "[`metric`][metric_event]"] events to a blackhole that simply discards data, designed for testing and benchmarking purposes.<br />`guarantee: best_effort` |
| [**`console`**](sinks/console.md) | Streams ["[`log`][log_event]", "[`metric`][metric_event]"] events to the console, `STDOUT` or `STDERR`.<br />`guarantee: best_effort` |
| [**`elasticsearch`**](sinks/elasticsearch.md) | Batches and flushes ["[`log`][log_event]"] events to [Elasticsearch][elasticsearch] via the [`_bulk` API endpoint](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html).<br />`guarantee: best_effort` |
| [**`http`**](sinks/http.md) | Batches and flushes ["[`log`][log_event]"] events to a generic HTTP endpoint.<br />`guarantee: at_least_once` |
| [**`kafka`**](sinks/kafka.md) | Streams ["[`log`][log_event]"] events to [Apache Kafka][kafka] via the [Kafka protocol][kafka_protocol].<br />`guarantee: at_least_once` |
| [**`splunk_hec`**](sinks/splunk_hec.md) | Batches and flushes ["[`log`][log_event]"] events to a [Splunk HTTP Event Collector][splunk_hec].<br />`guarantee: at_least_once` |
| [**`tcp`**](sinks/tcp.md) | Streams ["[`log`][log_event]"] events to a TCP connection.<br />`guarantee: best_effort` |
| [**`vector`**](sinks/vector.md) | Streams ["[`log`][log_event]"] events to another downstream Vector instance.<br />`guarantee: best_effort` |

[+ request a new sink](https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature%2C%7B%3Atitle%3D%3E%22New+%60%3Cname%3E%60+sink%22%7D&title=New+%60%3Cname%3E%60+sink)

## How It Works

### Composition

The primary purpose of the configuration file is to compose pipelines. Pipelines are formed by connecting [sources][sources], [transforms][transforms], and [sinks][sinks] through the `inputs` option.

Notice in the above example each input references the `id` assigned to a previous source or transform.

### Config File Location

The location of your Vector configuration file depends on your [platform][platform] or [operating system][operating_system]. For most Linux based systems the file can be found at `/etc/vector/vector.toml`.

### Data Directory

Vector requires a `data_dir` value for on-disk operations. Currently, the only operation using this directory are Vector's on-disk buffers. Buffers, by default, are memory-based, but if you switch them to disk-based you'll need to specify a `data_directory`.

### Environment Variables

Vector will interpolate environment variables within your configuration file with the following syntax:

{% code-tabs %}
{% code-tabs-item title="vector.toml" %}
```coffeescript
[transforms.add_host]
    type = "add_fields"
    
    [transforms.add_host.fields]
        host = "${HOSTNAME}"
```
{% endcode-tabs-item %}
{% endcode-tabs %}

The entire `${HOSTNAME}` variable will be replaced, hence the requirement of quotes around the definition.

#### Escaping

You can escape environment variable by preceding them with a `$` character. For example `$${HOSTNAME}` will be treated _literally_ in the above environment variable example.

### Format

The Vector configuration file requires the [TOML][toml] format for it's simplicity, explicitness, and relaxed white-space parsing. For more information, please refer to the excellent [TOML documentation][toml].

### Value Types

All TOML values types are supported. For convenience this includes:

* [Strings](https://github.com/toml-lang/toml#string)
* [Integers](https://github.com/toml-lang/toml#integer)
* [Floats](https://github.com/toml-lang/toml#float)
* [Booleans](https://github.com/toml-lang/toml#boolean)
* [Offset Date-Times](https://github.com/toml-lang/toml#offset-date-time)
* [Local Date-Times](https://github.com/toml-lang/toml#local-date-time)
* [Local Dates](https://github.com/toml-lang/toml#local-date)
* [Local Times](https://github.com/toml-lang/toml#local-time)
* [Arrays](https://github.com/toml-lang/toml#array)
* [Tables](https://github.com/toml-lang/toml#table)


[log_event]: "../../../about/data-model.md#log"
[metric_event]: "../../../about/data-model.md#metric"
[grok]: "http://grokdebug.herokuapp.com/"
[lua]: "https://www.lua.org/"
[regex]: "https://en.wikipedia.org/wiki/Regular_expression"
[aws_cw_logs]: "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html"
[aws_kinesis_data_streams]: "https://aws.amazon.com/kinesis/data-streams/"
[aws_s3]: "https://aws.amazon.com/s3/"
[elasticsearch]: "https://www.elastic.co/products/elasticsearch"
[kafka]: "https://kafka.apache.org/"
[kafka_protocol]: "https://kafka.apache.org/protocol"
[splunk_hec]: "http://dev.splunk.com/view/event-collector/SP-CAAAE6M"
[sources]: "../../../usage/configuration/sources"
[transforms]: "../../../usage/configuration/transforms"
[sinks]: "../../../usage/configuration/sinks"
[platform]: "../../setup/installation/platforms/"
[operating_system]: "../../setup/installation/operating_systems/"
[toml]: "https://github.com/toml-lang/toml"

