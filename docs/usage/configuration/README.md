---
description: Vector configuration
---

<!---
!!!WARNING!!!!

This file is autogenerated! Please do not manually edit this file.
Instead, please modify the contents of `scripts/config_schema.toml`.
-->


# Configuration

![](../../assets/configure.svg)

This section covers configuring Vector and creating [pipelines](../../about/concepts.md#pipelines) like the one shown above. Vector requires only a _single_ [TOML](https://github.com/toml-lang/toml) configurable file, which you can specify via the [`--config` flag](../administration/starting.md#options) when [starting](../administration/starting.md) vector:

```bash
vector --config /etc/vector/vector.toml
```

## Example

{% code-tabs %}
{% code-tabs-item title="vector.toml" %}
```coffeescript
data_dir = "/var/lib/vector"

# Ingest data by tailing one or more files
[sources.apache_logs]
    type         = "file"
    include      = ["/var/log/apache2/*.log"]
    ignore_older = 86400 # 1 day

# Structure and parse the data
[transforms.apache_parser]
    inputs        = ["apache_logs"]
  type            = "regex_parser"
  regex           = '^(?P<host>[w.]+) - (?P<user>[w]+) (?P<bytes_in>[d]+) [(?P<timestamp>.*)] "(?P<method>[w]+) (?P<path>.*)" (?P<status>[d]+) (?P<bytes_out>[d]+)$'

# Sample the data to save on cost
[transforms.apache_sampler]
    inputs       = ["apache_parser"]
    type         = "sampler"
    hash_field   = "request_id" # sample _entire_ requests
    rate         = 10 # only keep 10%

# Send structured data to a short-term storage
[sinks.es_cluster]
    inputs       = ["apache_sampler"]
    type         = "elasticsearch"
    host         = "79.12.221.222:9200"
    doc_type     = "_doc"

# Send structured data to a cost-effective long-term storage
[sinks.s3_archives]
    inputs       = ["apache_parser"] # don't sample
    type         = "aws_s3"
    region       = "us-east-1"
    bucket       = "my_log_archives"
    batch_size   = 10000000 # 10mb uncompressed
    gzip         = true
    encoding     = "ndjson"
```
{% endcode-tabs-item %}
{% endcode-tabs %}

## Global Options

| Key  | Type  | Description |
| :--- | :---: | :---------- |
| `data_dir` | `string` | The directory used for persisting Vector state, such as on-disk buffers. Please make sure the Vector project has write permissions to this dir. See [Data Directory](#data-directory) for more info.<br />`no default` `example: "/var/lib/vector"` |

## Sources

| Name  | Description |
| :---  | :---------- |
| [**`file`**][file_source] | Ingests data through one or more local files. |
| [**`statsd`**][statsd_source] | Ingests data through the StatsD UDP protocol. |
| [**`stdin`**][stdin_source] | Ingests data through standard input (STDIN). |
| [**`syslog`**][syslog_source] | Ingests data through the Syslog 5424 protocol. |
| [**`tcp`**][tcp_source] | Ingests data through the TCP protocol. |
| [**`vector`**][vector_source] | Ingests data through another upstream Vector instance. |

[+ request a new source](https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature%2C%7B%3Atitle%3D%3E%22New+%60%3Cname%3E%60+source%22%7D&title=New+%60%3Cname%3E%60+source)

## Transforms

| Name  | Description |
| :---  | :---------- |
| [**`add_fields`**][add_fields_transform] | Allows you to add one or more fields. |
| [**`field_filter`**][field_filter_transform] | Allows you to filter events by a field's value. |
| [**`grok_parser`**][grok_parser_transform] | Allows you to parse a field value with [Grok][grok]. |
| [**`json_parser`**][json_parser_transform] | Allows you to parse a field value as JSON. |
| [**`lua`**][lua_transform] | Allows you to transform events with a full embedded [Lua][lua] engine. |
| [**`regex_parser`**][regex_parser_transform] | Allows you to parse a field's value with a [Regular Expression][regex]. |
| [**`remove_fields`**][remove_fields_transform] | Allows you to remove one or more event fields. |
| [**`sampler`**][sampler_transform] | Allows you to sample events with a configurable rate. |
| [**`tokenizer`**][tokenizer_transform] | Allows you to tokenize a field's value by splitting on white space, ignoring special wrapping characters, and zipping the tokens into ordered field names. |

[+ request a new transform](https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature%2C%7B%3Atitle%3D%3E%22New+%60%3Cname%3E%60+transform%22%7D&title=New+%60%3Cname%3E%60+transform)

## Sinks

| Name  | Description |
| :---  | :---------- |
| [**`aws_cloudwatch_logs`**][aws_cloudwatch_logs_sink] | Batches and flushes events to [AWS CloudWatch Logs][aws_cw_logs] via the [`PutLogEvents` API endpoint](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutLogEvents.html). |
| [**`aws_kinesis_streams`**][aws_kinesis_streams_sink] | Batches and flushes events to [AWS Kinesis Data Stream][aws_kinesis_data_streams] via the [`PutRecords` API endpoint](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html). |
| [**`aws_s3`**][aws_s3_sink] | Batches and flushes events to [AWS S3][aws_s3] via the [`PutObject` API endpoint](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html). |
| [**`blackhole`**][blackhole_sink] | Streams events to a blackhole that simply discards data, designed for testing and benchmarking purposes. |
| [**`console`**][console_sink] | Streams events to the console, `STDOUT` or `STDERR`. |
| [**`elasticsearch`**][elasticsearch_sink] | Batches and flushes events to [Elasticsearch][elasticsearch] via the [`_bulk` API endpoint](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html). |
| [**`http`**][http_sink] | Batches and flushes events to a generic HTTP endpoint. |
| [**`kafka`**][kafka_sink] | Streams events to [Apache Kafka][kafka] via the [Kafka protocol][kafka_protocol]. |
| [**`splunk_hec`**][splunk_hec_sink] | Batches and flushes events to a [Splunk HTTP Event Collector][splunk_hec]. |
| [**`tcp`**][tcp_sink] | Streams events to a TCP connection. |
| [**`vector`**][vector_sink] | Streams events to another downstream Vector instance. |

[+ request a new sink](https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature%2C%7B%3Atitle%3D%3E%22New+%60%3Cname%3E%60+sink%22%7D&title=New+%60%3Cname%3E%60+sink)

## How It Works

### Composition

The primary purpose of the configuration file is to compose pipelines. Pipelines are formed by connecting [sources][sources], [transforms][transforms], and [sinks][sinks] through the `inputs` option.

Notice in the above example each input references the `id` assigned to a previous source or transform.

### Config File Location

The location of your Vector configuration file depends on your [platform][platform] or [operating system][operating_system]. For most Linux based systems the file can be found at `/etc/vector/vector.toml`.

### Data Directory

Vector requires a `data_dir` value for on-disk operations. Currently, the only operation using this directory are Vector's on-disk buffers. Buffers, by default, are memory-based, but if you switch them to disk-based you'll need to specify a `data_directory`.

### Environment Variables

Vector will interpolate environment variables within your configuration file with the following syntax:

{% code-tabs %}
{% code-tabs-item title="vector.toml" %}
```coffeescript
[transforms.add_host]
    type = "add_fields"
    
    [transforms.add_host.fields]
        host = "${HOSTNAME}"
```
{% endcode-tabs-item %}
{% endcode-tabs %}

The entire `${HOSTNAME}` variable will be replaced, hence the requirement of quotes around the definition.

#### Escaping

You can escape environment variable by preceding them with a `$` character. For example `$${HOSTNAME}` will be treated _literally_ in the above environment variable example.

### Format

The Vector configuration file requires the [TOML][toml] format for it's simplicity, explicitness, and relaxed white-space parsing. For more information, please refer to the excellent [TOML documentation][toml].

### Value Types

All TOML values types are supported. For convenience this includes:

* [Strings](https://github.com/toml-lang/toml#string)
* [Integers](https://github.com/toml-lang/toml#integer)
* [Floats](https://github.com/toml-lang/toml#float)
* [Booleans](https://github.com/toml-lang/toml#boolean)
* [Offset Date-Times](https://github.com/toml-lang/toml#offset-date-time)
* [Local Date-Times](https://github.com/toml-lang/toml#local-date-time)
* [Local Dates](https://github.com/toml-lang/toml#local-date)
* [Local Times](https://github.com/toml-lang/toml#local-time)
* [Arrays](https://github.com/toml-lang/toml#array)
* [Tables](https://github.com/toml-lang/toml#table)


[file_source]: "../../../usage/configuration/sources/file.md"
[statsd_source]: "../../../usage/configuration/sources/statsd.md"
[stdin_source]: "../../../usage/configuration/sources/stdin.md"
[syslog_source]: "../../../usage/configuration/sources/syslog.md"
[tcp_source]: "../../../usage/configuration/sources/tcp.md"
[vector_source]: "../../../usage/configuration/sources/vector.md"
[add_fields_transform]: "../../../usage/configuration/transforms/add_fields.md"
[field_filter_transform]: "../../../usage/configuration/transforms/field_filter.md"
[grok_parser_transform]: "../../../usage/configuration/transforms/grok_parser.md"
[grok]: "http://grokdebug.herokuapp.com/"
[json_parser_transform]: "../../../usage/configuration/transforms/json_parser.md"
[lua_transform]: "../../../usage/configuration/transforms/lua.md"
[lua]: "https://www.lua.org/"
[regex_parser_transform]: "../../../usage/configuration/transforms/regex_parser.md"
[regex]: "https://en.wikipedia.org/wiki/Regular_expression"
[remove_fields_transform]: "../../../usage/configuration/transforms/remove_fields.md"
[sampler_transform]: "../../../usage/configuration/transforms/sampler.md"
[tokenizer_transform]: "../../../usage/configuration/transforms/tokenizer.md"
[aws_cloudwatch_logs_sink]: "../../../usage/configuration/sinks/aws_cloudwatch_logs.md"
[aws_cw_logs]: "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html"
[aws_kinesis_streams_sink]: "../../../usage/configuration/sinks/aws_kinesis_streams.md"
[aws_kinesis_data_streams]: "https://aws.amazon.com/kinesis/data-streams/"
[aws_s3_sink]: "../../../usage/configuration/sinks/aws_s3.md"
[aws_s3]: "https://aws.amazon.com/s3/"
[blackhole_sink]: "../../../usage/configuration/sinks/blackhole.md"
[console_sink]: "../../../usage/configuration/sinks/console.md"
[elasticsearch_sink]: "../../../usage/configuration/sinks/elasticsearch.md"
[elasticsearch]: "https://www.elastic.co/products/elasticsearch"
[http_sink]: "../../../usage/configuration/sinks/http.md"
[kafka_sink]: "../../../usage/configuration/sinks/kafka.md"
[kafka]: "https://kafka.apache.org/"
[kafka_protocol]: "https://kafka.apache.org/protocol"
[splunk_hec_sink]: "../../../usage/configuration/sinks/splunk_hec.md"
[splunk_hec]: "http://dev.splunk.com/view/event-collector/SP-CAAAE6M"
[tcp_sink]: "../../../usage/configuration/sinks/tcp.md"
[vector_sink]: "../../../usage/configuration/sinks/vector.md"
[sources]: "../../../usage/configuration/sources"
[transforms]: "../../../usage/configuration/transforms"
[sinks]: "../../../usage/configuration/sinks"
[platform]: "../../setup/installation/platforms/"
[operating_system]: "../../setup/installation/operating_systems/"
[toml]: "https://github.com/toml-lang/toml"

