---
description: Push or expose events in Vector to external systems
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the template located at:

     scripts/generate/templates/docs/usage/configuration/sinks/README.md.erb
-->

# Sinks

![][images.sinks]

Sinks are last in the [pipeline][docs.pipelines], responsible for sending
[events][docs.event] downstream. These can be service specific sinks, such as
[`vector`][docs.vector_sink], [`elasticsearch`][docs.elasticsearch_sink], and
[`s3`][docs.aws_s3_sink], or generic protocol sinks like
[`http`][docs.http_sink] or [`tcp`][docs.tcp_sink].

| Name  | Description |
|:------|:------------|
| [**`aws_cloudwatch_logs`**][docs.aws_cloudwatch_logs_sink] | [Batches](#buffers-and-batches) [`log`][docs.log_event] events to [AWS CloudWatch Logs][url.aws_cw_logs] via the [`PutLogEvents` API endpoint](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutLogEvents.html). |
| [**`aws_kinesis_streams`**][docs.aws_kinesis_streams_sink] | [Batches](#buffers-and-batches) [`log`][docs.log_event] events to [AWS Kinesis Data Stream][url.aws_kinesis_data_streams] via the [`PutRecords` API endpoint](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html). |
| [**`aws_s3`**][docs.aws_s3_sink] | [Batches](#buffers-and-batches) [`log`][docs.log_event] events to [AWS S3][url.aws_s3] via the [`PutObject` API endpoint](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html). |
| [**`blackhole`**][docs.blackhole_sink] | [Streams](#streaming) [`log`][docs.log_event] and [`metric`][docs.metric_event] events to a blackhole that simply discards data, designed for testing and benchmarking purposes. |
| [**`clickhouse`**][docs.clickhouse_sink] | [Batches](#buffers-and-batches) [`log`][docs.log_event] events to [Clickhouse][url.clickhouse] via the [`HTTP` Interface][url.clickhouse_http]. |
| [**`console`**][docs.console_sink] | [Streams](#streaming) [`log`][docs.log_event] and [`metric`][docs.metric_event] events to the console, `STDOUT` or `STDERR`. |
| [**`elasticsearch`**][docs.elasticsearch_sink] | [Batches](#buffers-and-batches) [`log`][docs.log_event] events to [Elasticsearch][url.elasticsearch] via the [`_bulk` API endpoint](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html). |
| [**`http`**][docs.http_sink] | [Batches](#buffers-and-batches) [`log`][docs.log_event] events to a generic HTTP endpoint. |
| [**`kafka`**][docs.kafka_sink] | [Streams](#streaming) [`log`][docs.log_event] events to [Apache Kafka][url.kafka] via the [Kafka protocol][url.kafka_protocol]. |
| [**`prometheus`**][docs.prometheus_sink] | [Exposes](#exposing-and-scraping) [`metric`][docs.metric_event] events to [Prometheus][url.prometheus] metrics service. |
| [**`splunk_hec`**][docs.splunk_hec_sink] | [Batches](#buffers-and-batches) [`log`][docs.log_event] events to a [Splunk HTTP Event Collector][url.splunk_hec]. |
| [**`tcp`**][docs.tcp_sink] | [Streams](#streaming) [`log`][docs.log_event] events to a TCP connection. |
| [**`vector`**][docs.vector_sink] | [Streams](#streaming) [`log`][docs.log_event] events to another downstream Vector instance. |

[+ request a new sink][url.new_sink]

## Healthchecks

Sinks may implement a healthcheck as a means for validating their configuration
against the envionment and external systems. Ideally, this allows the system to
inform users of problems such as insufficient credentials, unreachable
endpoints, non-existant tables, etc. They're not perfect, however, since it's
impossible to exhaustively check for issues that may happen at runtime.

### Guidelines for writing healthchecks

When implementing healthchecks, we prefer false positives to false negatives.
This means we would prefer that a healthcheck pass and the sink then fail than
to have the healthcheck fail when the sink would have been able to run
successfully.

A common cause of false negatives in healthchecks is performing an operation
that the sink itself does not need. For example, listing all of the available S3
buckets and checking that the configured bucket is in that list. The S3 sink
doesn't need the ability to list all buckets, and a user that knows that may not
have given it permission to do so. In that case, the healthcheck will fail due
to bad credentials even through its credentials are sufficient for normal
operation.

This leads to a general strategy of mimicking what the sink itself does.
Unfortunately, the fact that healthchecks don't have real events available to
them leads to some limitations here. The most obvious example of this is with
sinks where the exact target of a write depends on the value of some field in
the event (e.g. an interpolated Kinesis stream name). It also pops up for sinks
where incoming events are expected to conform to a specific schema. In both
cases, random test data is reasonably likely to trigger a potentially false
negative result. Even in simpler cases, we need to think about the effects of
writing test data and whether the user would find that surprising or invasive.
The answer usually depends on the system we're interfacing with.

In some cases, like the Kinesis example above, the right thing to do might be
nothing at all. If we require dynamic information to figure out what entity
(i.e. Kinesis stream in this case) that we're even dealing with, odds are very
low that we'll be able to come up with a way to meaningfully validate that it's
in working order. It's perfectly valid to have a healthcheck that falls back to
doing nothing when there is a data dependency like this.

With all that in mind, here is a simple checklist to go over when writing a new
healthcheck:

- [ ] Does this check perform different fallible operations from the sink itself?
- [ ] Does this check have side effects the user would consider undesirable (e.g. data pollution)?
- [ ] Are there situations where this check would fail but the sink would operate normally?

Not all of the answers need to be a hard "no", but we should think about the
likelihood that any "yes" would lead to false negatives and balance that against
the usefulness of the check as a whole for finding problems. Because we have the
option to disable individual healthchecks, there's an escape hatch for users
that fall into a false negative circumstance. Our goal should be to minimize the
likelihood of users needing to pull that lever while still making a good effort
to detect common problems.


[docs.aws_cloudwatch_logs_sink]: ../../../usage/configuration/sinks/aws_cloudwatch_logs.md
[docs.aws_kinesis_streams_sink]: ../../../usage/configuration/sinks/aws_kinesis_streams.md
[docs.aws_s3_sink]: ../../../usage/configuration/sinks/aws_s3.md
[docs.blackhole_sink]: ../../../usage/configuration/sinks/blackhole.md
[docs.clickhouse_sink]: ../../../usage/configuration/sinks/clickhouse.md
[docs.console_sink]: ../../../usage/configuration/sinks/console.md
[docs.elasticsearch_sink]: ../../../usage/configuration/sinks/elasticsearch.md
[docs.event]: ../../../about/data-model/README.md#event
[docs.http_sink]: ../../../usage/configuration/sinks/http.md
[docs.kafka_sink]: ../../../usage/configuration/sinks/kafka.md
[docs.log_event]: ../../../about/data-model/log.md
[docs.metric_event]: ../../../about/data-model/metric.md
[docs.pipelines]: ../../../usage/configuration/README.md#composition
[docs.prometheus_sink]: ../../../usage/configuration/sinks/prometheus.md
[docs.splunk_hec_sink]: ../../../usage/configuration/sinks/splunk_hec.md
[docs.tcp_sink]: ../../../usage/configuration/sinks/tcp.md
[docs.vector_sink]: ../../../usage/configuration/sinks/vector.md
[images.sinks]: ../../../assets/sinks.svg
[url.aws_cw_logs]: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html
[url.aws_kinesis_data_streams]: https://aws.amazon.com/kinesis/data-streams/
[url.aws_s3]: https://aws.amazon.com/s3/
[url.clickhouse]: https://clickhouse.yandex/
[url.clickhouse_http]: https://clickhouse.yandex/docs/en/interfaces/http/
[url.elasticsearch]: https://www.elastic.co/products/elasticsearch
[url.kafka]: https://kafka.apache.org/
[url.kafka_protocol]: https://kafka.apache.org/protocol
[url.new_sink]: https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature
[url.prometheus]: https://prometheus.io/
[url.splunk_hec]: http://dev.splunk.com/view/event-collector/SP-CAAAE6M
