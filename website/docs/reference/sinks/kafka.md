---
delivery_guarantee: "at_least_once"
component_title: "Kafka"
description: "The Vector `kafka` sink streams `log` events to Apache Kafka via the Kafka protocol."
event_types: ["log"]
function_category: "transmit"
issues_url: https://github.com/timberio/vector/issues?q=is%3Aopen+is%3Aissue+label%3A%22sink%3A+kafka%22
min_version: "0.8"
operating_systems: ["Linux","MacOS","Windows"]
service_name: "Kafka"
sidebar_label: "kafka|[\"log\"]"
source_url: https://github.com/timberio/vector/tree/master/src/sinks/kafka.rs
status: "prod-ready"
title: "Kafka Sink"
unsupported_operating_systems: []
---

The Vector `kafka` sink
[streams](#streaming) [`log`][docs.data-model.log] events to [Apache
Kafka][urls.kafka] via the [Kafka protocol][urls.kafka_protocol].

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the template located at:

     website/docs/reference/sinks/kafka.md.erb
-->

## Configuration

import Tabs from '@theme/Tabs';

<Tabs
  block={true}
  defaultValue="common"
  values={[{"label":"Common","value":"common"},{"label":"Advanced","value":"advanced"}]}>

import TabItem from '@theme/TabItem';

<TabItem value="common">

import CodeHeader from '@site/src/components/CodeHeader';

<CodeHeader fileName="vector.toml" learnMoreUrl="/docs/setup/configuration/"/ >

```toml
[sinks.my_sink_id]
  # General
  type = "kafka" # required
  inputs = ["my-source-id"] # required
  bootstrap_servers = "10.14.22.123:9092,10.14.23.332:9092" # required
  key_field = "user_id" # required
  topic = "topic-1234" # required
  compression = "none" # optional, default
  healthcheck = true # optional, default

  # Encoding
  encoding.codec = "json" # required
```

</TabItem>
<TabItem value="advanced">

<CodeHeader fileName="vector.toml" learnMoreUrl="/docs/setup/configuration/"/ >

```toml
[sinks.my_sink_id]
  # General
  type = "kafka" # required
  inputs = ["my-source-id"] # required
  bootstrap_servers = "10.14.22.123:9092,10.14.23.332:9092" # required
  key_field = "user_id" # required
  topic = "topic-1234" # required
  compression = "none" # optional, default
  healthcheck = true # optional, default
  message_timeout_ms = 300000 # optional, default
  socket_timeout_ms = 60000 # optional, default

  # Advanced
  librdkafka_options."client.id" = "${ENV_VAR}" # example
  librdkafka_options."fetch.error.backoff.ms" = "1000" # example
  librdkafka_options."socket.send.buffer.bytes" = "100" # example

  # Buffer
  buffer.max_size = 104900000 # required, bytes, required when type = "disk"
  buffer.type = "memory" # optional, default
  buffer.max_events = 500 # optional, default, events, relevant when type = "memory"
  buffer.when_full = "block" # optional, default

  # Encoding
  encoding.codec = "json" # required
  encoding.except_fields = ["timestamp", "message", "host"] # optional, no default
  encoding.only_fields = ["timestamp", "message", "host"] # optional, no default
  encoding.timestamp_format = "rfc3339" # optional, default

  # TLS
  tls.ca_path = "/path/to/certificate_authority.crt" # optional, no default
  tls.crt_path = "/path/to/host_certificate.crt" # optional, no default
  tls.enabled = false # optional, default
  tls.key_pass = "${KEY_PASS_ENV_VAR}" # optional, no default
  tls.key_path = "/path/to/host_certificate.key" # optional, no default
```

</TabItem>
</Tabs>

## Requirements

import Alert from '@site/src/components/Alert';

<Alert icon={false} type="danger" classNames="list--warnings">

* Kafka version >= 0.8 is required.


</Alert>

## Options

import Fields from '@site/src/components/Fields';

import Field from '@site/src/components/Field';

<Fields filters={true}>


<Field
  common={true}
  defaultValue={null}
  enumValues={null}
  examples={["10.14.22.123:9092,10.14.23.332:9092"]}
  groups={[]}
  name={"bootstrap_servers"}
  path={null}
  relevantWhen={null}
  required={true}
  templateable={false}
  type={"string"}
  unit={null}
  >

### bootstrap_servers

A comma-separated list of host and port pairs that are the addresses of the
Kafka brokers in a "bootstrap" Kafka cluster that a Kafka client connects to
initially to bootstrap itself.




</Field>


<Field
  common={false}
  defaultValue={null}
  enumValues={null}
  examples={[]}
  groups={[]}
  name={"buffer"}
  path={null}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"table"}
  unit={null}
  >

### buffer

Configures the sink specific buffer behavior.



<Fields filters={false}>


<Field
  common={true}
  defaultValue={500}
  enumValues={null}
  examples={[500]}
  groups={[]}
  name={"max_events"}
  path={"buffer"}
  relevantWhen={{"type":"memory"}}
  required={false}
  templateable={false}
  type={"int"}
  unit={"events"}
  >

#### max_events

The maximum number of [events][docs.data-model] allowed in the buffer.




</Field>


<Field
  common={false}
  defaultValue={null}
  enumValues={null}
  examples={[104900000]}
  groups={[]}
  name={"max_size"}
  path={"buffer"}
  relevantWhen={{"type":"disk"}}
  required={true}
  templateable={false}
  type={"int"}
  unit={"bytes"}
  >

#### max_size

The maximum size of the buffer on the disk.




</Field>


<Field
  common={true}
  defaultValue={"memory"}
  enumValues={{"memory":"Stores the sink's buffer in memory. This is more performant, but less durable. Data will be lost if Vector is restarted forcefully.","disk":"Stores the sink's buffer on disk. This is less performant, but durable. Data will not be lost between restarts."}}
  examples={["memory","disk"]}
  groups={[]}
  name={"type"}
  path={"buffer"}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"string"}
  unit={null}
  >

#### type

The buffer's type and storage mechanism.




</Field>


<Field
  common={false}
  defaultValue={"block"}
  enumValues={{"block":"Applies back pressure when the buffer is full. This prevents data loss, but will cause data to pile up on the edge.","drop_newest":"Drops new data as it's received. This data is lost. This should be used when performance is the highest priority."}}
  examples={["block","drop_newest"]}
  groups={[]}
  name={"when_full"}
  path={"buffer"}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"string"}
  unit={null}
  >

#### when_full

The behavior when the buffer becomes full.




</Field>


</Fields>

</Field>


<Field
  common={true}
  defaultValue={"none"}
  enumValues={{"none":"No compression","gzip":"[Gzip](https://www.gnu.org/software/gzip/) standard DEFLATE compression","lz4":"High speed [LZ4 compression](https://lz4.github.io/lz4/)","snappy":"High speed [Snappy compression](https://google.github.io/snappy/), developed by Google. Slower than LZ4 but higher compression.","zstd":"[Zstandard compression](https://zstd.net), developed at Facebook. Faster than gzip at similar compression ratios."}}
  examples={["none","gzip","lz4","snappy","zstd"]}
  groups={[]}
  name={"compression"}
  path={null}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"string"}
  unit={null}
  >

### compression

Compression codec to use for compressing message sets




</Field>


<Field
  common={true}
  defaultValue={null}
  enumValues={null}
  examples={[]}
  groups={[]}
  name={"encoding"}
  path={null}
  relevantWhen={null}
  required={true}
  templateable={false}
  type={"table"}
  unit={null}
  >

### encoding

Configures the encoding specific sink behavior.



<Fields filters={false}>


<Field
  common={true}
  defaultValue={null}
  enumValues={{"json":"Each event is encoded into JSON and the payload is represented as a JSON array.","text":"Each event is encoded into text via the [`message`](#message) key and the payload is new line delimited."}}
  examples={["json","text"]}
  groups={[]}
  name={"codec"}
  path={"encoding"}
  relevantWhen={null}
  required={true}
  templateable={false}
  type={"string"}
  unit={null}
  >

#### codec

The encoding codec used to serialize the events before outputting.




</Field>


<Field
  common={false}
  defaultValue={null}
  enumValues={null}
  examples={[["timestamp","message","host"]]}
  groups={[]}
  name={"except_fields"}
  path={"encoding"}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"[string]"}
  unit={null}
  >

#### except_fields

Prevent the sink from encoding the specified labels.




</Field>


<Field
  common={false}
  defaultValue={null}
  enumValues={null}
  examples={[["timestamp","message","host"]]}
  groups={[]}
  name={"only_fields"}
  path={"encoding"}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"[string]"}
  unit={null}
  >

#### only_fields

Limit the sink to only encoding the specified labels.




</Field>


<Field
  common={false}
  defaultValue={"rfc3339"}
  enumValues={{"rfc3339":"Format as an RFC3339 string","unix":"Format as a unix timestamp, can be parsed as a Clickhouse DateTime"}}
  examples={["rfc3339","unix"]}
  groups={[]}
  name={"timestamp_format"}
  path={"encoding"}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"string"}
  unit={null}
  >

#### timestamp_format

How to format event timestamps.




</Field>


</Fields>

</Field>


<Field
  common={true}
  defaultValue={true}
  enumValues={null}
  examples={[true,false]}
  groups={[]}
  name={"healthcheck"}
  path={null}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"bool"}
  unit={null}
  >

### healthcheck

Enables/disables the sink healthcheck upon start.

 See [Health Checks](#health-checks) for more info.


</Field>


<Field
  common={true}
  defaultValue={null}
  enumValues={null}
  examples={["user_id"]}
  groups={[]}
  name={"key_field"}
  path={null}
  relevantWhen={null}
  required={true}
  templateable={false}
  type={"string"}
  unit={null}
  >

### key_field

The log field name to use for the topic key. If unspecified, the key will be
randomly generated. If the field does not exist on the log, a blank value will
be used.




</Field>


<Field
  common={false}
  defaultValue={null}
  enumValues={null}
  examples={[]}
  groups={[]}
  name={"librdkafka_options"}
  path={null}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"table"}
  unit={null}
  >

### librdkafka_options

Advanced options. See [the [`librdkafka`](#librdkafka) documentation][urls.lib_rdkafka_config]
for details.



<Fields filters={false}>


<Field
  common={false}
  defaultValue={null}
  enumValues={null}
  examples={[{"client.id":"${ENV_VAR}"},{"fetch.error.backoff.ms":"1000"},{"socket.send.buffer.bytes":"100"}]}
  groups={[]}
  name={"`[field-name]`"}
  path={"librdkafka_options"}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"string"}
  unit={null}
  >

#### `[field-name]`

The options and their values. Accepts `string` values.




</Field>


</Fields>

</Field>


<Field
  common={false}
  defaultValue={300000}
  enumValues={null}
  examples={[150000,450000]}
  groups={[]}
  name={"message_timeout_ms"}
  path={null}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"int"}
  unit={null}
  >

### message_timeout_ms

Local message timeout.




</Field>


<Field
  common={false}
  defaultValue={60000}
  enumValues={null}
  examples={[30000,90000]}
  groups={[]}
  name={"socket_timeout_ms"}
  path={null}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"int"}
  unit={null}
  >

### socket_timeout_ms

Default timeout for network requests.




</Field>


<Field
  common={false}
  defaultValue={null}
  enumValues={null}
  examples={[]}
  groups={[]}
  name={"tls"}
  path={null}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"table"}
  unit={null}
  >

### tls

Configures the TLS options for connections from this sink.



<Fields filters={false}>


<Field
  common={true}
  defaultValue={false}
  enumValues={null}
  examples={[false,true]}
  groups={[]}
  name={"enabled"}
  path={"tls"}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"bool"}
  unit={null}
  >

#### enabled

Enable TLS during connections to the remote.




</Field>


<Field
  common={false}
  defaultValue={null}
  enumValues={null}
  examples={["/path/to/certificate_authority.crt"]}
  groups={[]}
  name={"ca_path"}
  path={"tls"}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"string"}
  unit={null}
  >

#### ca_path

Absolute path to an additional CA certificate file, in DER or PEM format
(X.509).




</Field>


<Field
  common={true}
  defaultValue={null}
  enumValues={null}
  examples={["/path/to/host_certificate.crt"]}
  groups={[]}
  name={"crt_path"}
  path={"tls"}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"string"}
  unit={null}
  >

#### crt_path

Absolute path to a certificate file used to identify this connection, in DER or
PEM format (X.509) or PKCS#12. If this is set and is not a PKCS#12 archive,
`key_path` must also be set.




</Field>


<Field
  common={false}
  defaultValue={null}
  enumValues={null}
  examples={["${KEY_PASS_ENV_VAR}","PassWord1"]}
  groups={[]}
  name={"key_pass"}
  path={"tls"}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"string"}
  unit={null}
  >

#### key_pass

Pass phrase used to unlock the encrypted key file. This has no effect unless
`key_path` is set.




</Field>


<Field
  common={true}
  defaultValue={null}
  enumValues={null}
  examples={["/path/to/host_certificate.key"]}
  groups={[]}
  name={"key_path"}
  path={"tls"}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"string"}
  unit={null}
  >

#### key_path

Absolute path to a certificate key file used to identify this connection, in
DER or PEM format (PKCS#8). If this is set, [`crt_path`](#crt_path) must also be set.




</Field>


</Fields>

</Field>


<Field
  common={true}
  defaultValue={null}
  enumValues={null}
  examples={["topic-1234"]}
  groups={[]}
  name={"topic"}
  path={null}
  relevantWhen={null}
  required={true}
  templateable={false}
  type={"string"}
  unit={null}
  >

### topic

The Kafka topic name to write events to.




</Field>


</Fields>

## How It Works

### AWS Authentication

Vector checks for AWS credentials in the following order:

1. Environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
2. The [`credential_process` command][urls.aws_credential_process] in the AWS config file. (usually located at `~/.aws/config`)
3. The [AWS credentials file][urls.aws_credentials_file]. (usually located at `~/.aws/credentials`)
4. The [IAM instance profile][urls.iam_instance_profile]. (will only work if running on an EC2 instance with an instance profile/role)

If credentials are not found the [healtcheck](#healthchecks) will fail and an
error will be [logged][docs.monitoring#logs].

#### Obtaining an access key

In general, we recommend using instance profiles/roles whenever possible. In
cases where this is not possible you can generate an AWS access key for any user
within your AWS account. AWS provides a [detailed guide][urls.aws_access_keys] on
how to do this.

### Buffers

import SVG from 'react-inlinesvg';

<SVG src="/img/buffers.svg" />

The `kafka` sink buffers events as shown in
the diagram above. This helps to smooth out data processing if the downstream
service applies backpressure. Buffers are controlled via the
[`buffer.*`](#buffer) options.

### Environment Variables

Environment variables are supported through all of Vector's configuration.
Simply add `${MY_ENV_VAR}` in your Vector configuration file and the variable
will be replaced before being evaluated.

You can learn more in the
[Environment Variables][docs.configuration#environment-variables] section.

### Health Checks

Health checks ensure that the downstream service is accessible and ready to
accept data. This check is performed upon sink initialization.
If the health check fails an error will be logged and Vector will proceed to
start.

#### Require Health Checks

If you'd like to exit immediately upon a health check failure, you can
pass the `--require-healthy` flag:

```bash
vector --config /etc/vector/vector.toml --require-healthy
```

#### Disable Health Checks

If you'd like to disable health checks for this sink you can set the
`healthcheck` option to `false`.

### Streaming

The `kafka` sink streams data on a real-time
event-by-event basis. It does not batch data.

### librdkafka

The `kafka` sink uses [`lib_rdkafka`][urls.lib_rdkafka] under the hood. This
is a battle tested, performant, and reliabile library that facilitates
communication with Kafka. And because Vector produces static MUSL builds,
this dependency is packaged with Vector, meaning you do not need to install it.


[docs.configuration#environment-variables]: /docs/setup/configuration/#environment-variables
[docs.data-model.log]: /docs/about/data-model/log/
[docs.data-model]: /docs/about/data-model/
[docs.monitoring#logs]: /docs/administration/monitoring/#logs
[urls.aws_access_keys]: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html
[urls.aws_credential_process]: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-sourcing-external.html
[urls.aws_credentials_file]: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html
[urls.iam_instance_profile]: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html
[urls.kafka]: https://kafka.apache.org/
[urls.kafka_protocol]: https://kafka.apache.org/protocol
[urls.lib_rdkafka]: https://github.com/edenhill/librdkafka
[urls.lib_rdkafka_config]: https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
