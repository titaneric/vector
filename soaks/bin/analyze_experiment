#!/usr/bin/env python3
import argparse
import numpy as np
import pandas as pd
import scipy.stats
import common

np.seterr(all='raise')

parser = argparse.ArgumentParser(description='t-test experiments with Welch method')
parser.add_argument('--baseline-sha', type=str, help='the sha of the baseline experiment')
parser.add_argument('--capture-dir', type=str, help='the directory to search for capture files')
parser.add_argument('--comparison-sha', type=str, help='the sha of the comparison experiment')
parser.add_argument('--erratic-soaks', type=str, default='', help='a comma separated list of known-erratic experiments, NOT TO BE USED LIGHTLY')
parser.add_argument('--mean-drift-percentage', type=float, default=8.87, help='the percentage of mean drift we allow in an experiment, expressed as a value from 0 to 100, default 9th percentile')
parser.add_argument('--p-value', type=float, default=0.1, help='the p-value for comparing with t-test results, the smaller the more certain')
parser.add_argument('--vector-cpus', type=int, help='the total number of CPUs given to vector during the experiment')
parser.add_argument('--warmup-seconds', type=int, default=30, help='the number of seconds to treat as warmup')
args = parser.parse_args()

erratic_soaks = args.erratic_soaks.split(',')

bytes_written = pd.concat(common.compute_throughput(
    common.open_captures(args.capture_dir,
                         'bytes_written',
                         unwanted_labels=['metric_name', 'metric_kind', 'target']),
    cpus = args.vector_cpus))
# Skip past warmup seconds samples, allowing for vector warmup to not factor
# into judgement.
bytes_written = bytes_written[(bytes_written.fetch_index > args.warmup_seconds) &
                              (bytes_written.throughput > 0.0)]

ttest_results = []
for exp in bytes_written.experiment.unique():
    baseline = bytes_written.loc[(bytes_written.experiment == exp) & (bytes_written.variant == 'baseline')]
    comparison = bytes_written.loc[(bytes_written.experiment == exp) & (bytes_written.variant == 'comparison')]

    baseline_mean = baseline.throughput.mean()
    baseline_stdev = baseline.throughput.std()
    comparison_mean = comparison.throughput.mean()
    comparison_stdev = comparison.throughput.std()
    diff =  comparison_mean - baseline_mean
    percent_change = round(((comparison_mean - baseline_mean) / baseline_mean) * 100, 2)

    baseline_outliers = common.total_outliers(baseline)
    comparison_outliers = common.total_outliers(comparison)

    # The t-test here is calculating whether the expected mean of our two
    # distributions is equal, or, put another way, whether the samples we have
    # here are from identical distributions. The higher the returned p-value by
    # ttest_ind the more likely it is that the samples _do_ have the same
    # expected mean.
    #
    # If the p-value is below our threshold then it is _unlikely_ that the two
    # samples actually have the same mean -- are from the same distribution --
    # and so there's some statistically interesting difference between the two
    # samples. For our purposes here that implies that performance has changed.
    res = scipy.stats.ttest_ind_from_stats(baseline_mean,
                                           baseline_stdev,
                                           len(baseline),
                                           comparison_mean,
                                           comparison_stdev,
                                           len(comparison),
                                           equal_var=False)
    ttest_results.append({'experiment': exp,
                          'Δ mean': diff.mean(),
                          'Δ mean %': percent_change,
                          'baseline mean': baseline_mean,
                          'baseline stdev': baseline_stdev,
                          'baseline outlier percentage': (baseline_outliers / len(baseline)) * 100,
                          'comparison mean': comparison_mean,
                          'comparison stdev': comparison_stdev,
                          'comparison outlier percentage': (comparison_outliers / len(comparison)) * 100,
                          'p-value': res.pvalue,
                          'erratic': exp in erratic_soaks
                          })

ttest_results = pd.DataFrame.from_records(ttest_results)

print(f'''
# Soak Test Results
Baseline: {args.baseline_sha}
Comparison: {args.comparison_sha}
Total Vector CPUs: {args.vector_cpus}

<details>
<summary>Explanation</summary>
<p>
A soak test is an integrated performance test for vector in a repeatable rig, with varying configuration for vector.
What follows is a statistical summary of a brief vector run for each configuration across SHAs given above.
The goal of these tests are to determine, quickly, if vector performance is changed and to what degree by a pull request.
Test units below are bytes/second/CPU, except for "skewness". The
further "skewness" is from 0.0 the more indication that vector lacks
consistency in behavior, making predictions of fitness in the field challenging.
</p>

<p>
The abbreviated table below, if present, lists those experiments that have experienced a
statistically significant change in their throughput performance between
baseline and comparision SHAs, with {(1.0 - args.p_value) * 100}% confidence. Negative values mean
that baseline is faster, positive comparison. Results that do not exhibit more than a ±{args.mean_drift_percentage}%
change in mean throughput are discarded. The abbreviated table will be omitted if no statistically
interesting changes are observed.
</p>
</details>
''')

p_value_violation = ttest_results['p-value'] < args.p_value
changes = ttest_results[p_value_violation].copy(deep=True)
changes['confidence'] = changes['p-value'].apply(common.confidence)
changes = changes.drop(labels=['p-value', 'baseline mean',
                               'baseline stdev', 'comparison mean',
                               'baseline outlier percentage',
                               'comparison outlier percentage',
                               'comparison stdev', 'erratic'], axis=1)
changes = changes.loc[~changes['experiment'].isin(erratic_soaks)]
changes = changes[changes['Δ mean %'].abs() > args.mean_drift_percentage].sort_values('Δ mean %', ascending=False)
changes['Δ mean'] = changes['Δ mean'].apply(common.human_bytes)
if len(changes) > 0:
    print(changes.to_markdown(index=False, tablefmt='github'))
else:
    print("No statistically interesting changes with confidence {}.".format(common.confidence(args.p_value)))

print()
print("<details>")
print("<summary>Fine details of change detection per experiment.</summary>")
print()
ttest_results = ttest_results.sort_values('Δ mean %', ascending=False)
ttest_results['Δ mean'] = ttest_results['Δ mean'].apply(common.human_bytes)
ttest_results['baseline mean'] = ttest_results['baseline mean'].apply(common.human_bytes)
ttest_results['baseline stdev'] = ttest_results['baseline stdev'].apply(common.human_bytes)
ttest_results['comparison mean'] = ttest_results['comparison mean'].apply(common.human_bytes)
ttest_results['comparison stdev'] = ttest_results['comparison stdev'].apply(common.human_bytes)
print(ttest_results.to_markdown(index=False, tablefmt='github'))
print("</details>")

print("<details>")
print("<summary>Fine details of each soak run.</summary>")
print()
describe = bytes_written.groupby(['experiment', 'variant', 'run_id']).throughput.describe(percentiles=[0.90, 0.95, 0.99])
describe = describe.rename(columns={'50%': 'average', '95%': 'p95', '90%': 'p90', '99%': 'p99'})
describe = describe.sort_values('mean', ascending=False)
describe['skewness'] = bytes_written.groupby(['experiment', 'variant', 'run_id']).throughput.skew()
describe['mean'] = describe['mean'].apply(common.human_bytes)
describe['std'] = describe['std'].apply(common.human_bytes)
describe['min'] = describe['min'].apply(common.human_bytes)
describe['average'] = describe['average'].apply(common.human_bytes)
describe['p90'] = describe['p90'].apply(common.human_bytes)
describe['p95'] = describe['p95'].apply(common.human_bytes)
describe['p99'] = describe['p99'].apply(common.human_bytes)
describe['max'] = describe['max'].apply(common.human_bytes)
print(describe.to_markdown(index=True,
                           tablefmt='github',
                           headers=['(experiment, variant, run_id)', 'total samples',
                                    'mean', 'std', 'min', 'average',
                                    'p90', 'p95', 'p99', 'max', 'skewness']))
print("</details>")
