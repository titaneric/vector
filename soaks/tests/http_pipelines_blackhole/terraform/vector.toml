data_dir = "/var/lib/vector"

##
## Sources
##

[sources.internal_metrics]
type = "internal_metrics"

[sources.logs]
type = "http"
address = "0.0.0.0:8282"
encoding = "text"

##
## Transforms
##

[transforms.preprocessing]
type = "remap"
inputs = ["logs"]
source = '''
., err = parse_json(.message)
.custom = {}
'''

[transforms.processing]
type = "pipelines"
inputs = ["preprocessing"]

[transforms.processing.logs]
order = [
    "nginx",
    "redis",
    "consul",
    "python",
    "rabbitmq",
    "zookeeper",
    "elasticsearch",
    "kafka",
    "couchdb",
    "docker",
    "datadog_agent",
    "ruby",
    "vault",
    "nginx_ingress_controller",
    "mysql",
    "kubernetes_cluster_autoscaler",
    "aws_alb_ingress_controller",
    "proxysql",
    "azure",
    "azure_web",
    "azure_storage",
    "azure_network",
    "azure_compute",
    "etcd",
    "glog_pipeline",
    "auth0",
    "kube_scheduler__glog_",
    "aws_ecs_agent",
    "nodejs",
    "postgresql",
    "cassandra",
    "apache_httpd",
    "azure_recovery_services",
    "c_",
    "web_browser_logs",
]

[transforms.processing.logs.pipelines.nginx]
name = "nginx"
filter.type = "datadog_search"
filter.source = "source:nginx"

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)","%{access.common} (%{number:duration:scale(1000000000)} )?\"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*","%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, %{data})?"],
aliases: {
"access.common": "%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)",
"access.combined": "%{access.common} (%{number:duration:scale(1000000000)} )?\"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*",
"error.format": "%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, %{data})?",
"_auth": "%{notSpace:http.auth:nullIf(\"-\")}",
"_bytes_written": "%{integer:network.bytes_written}",
"_client_ip": "%{ipOrHost:network.client.ip}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
"_url": "%{notSpace:http.url}",
"_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
"_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
"_referer": "%{notSpace:http.referer}",
"_status_code": "%{integer:http.status_code}",
"_method": "%{word:http.method}",
"_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}",
"_x_forwarded_for": "%{regex(\"[^\\\\\\\"]*\"):http._x_forwarded_for:nullIf(\"-\")}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
if exists(.custom.client) {
.custom.network.client.ip = .custom.client
}
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .request,
patterns: ["(?>%{_method} |)%{_url}(?> %{_version}|)"],
aliases: {
"request_parsing": "(?>%{_method} |)%{_url}(?> %{_version}|)",
"_method": "%{word:http.method}",
"_url": "%{notSpace:http.url}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.http.url); err == null) {
  .custom.http.url_details = details
}
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
if (details, err = parse_user_agent(.custom.http.useragent); err == null) {
  .custom.http.useragent_details = details
}
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
.timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@http.status_code:[200 TO 299]") {
.custom.http.status_category = "OK"
} else if match_datadog_query(., "@http.status_code:[300 TO 399]") {
.custom.http.status_category = "notice"
} else if match_datadog_query(., "@http.status_code:[400 TO 499]") {
.custom.http.status_category = "warning"
} else if match_datadog_query(., "@http.status_code:[500 TO 599]") {
.custom.http.status_category = "error"
}
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[transforms.processing.logs.pipelines.redis]
name = "redis"
filter.type = "datadog_search"
filter.source = "source:redis"

[[transforms.processing.logs.pipelines.redis.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{_pid}:%{_role} %{_date} %{_severity} %{data:message}"],
aliases: {
"default_format": "%{_pid}:%{_role} %{_date} %{_severity} %{data:message}",
"_date": "(%{date(\"dd MMM HH:mm:ss.SSS\"):date}|%{date(\"dd MMM yyyy HH:mm:ss.SSS\"):date})",
"_pid": "%{integer:pid}",
"_severity": "%{notSpace:severity}",
"_role": "%{word:role}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.redis.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
.timestamp = .custom.date
}
'''

[[transforms.processing.logs.pipelines.redis.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@severity: \".\"") {
.custom.redis.severity = "debug"
} else if match_datadog_query(., "@severity: \"-\"") {
.custom.redis.severity = "verbose"
} else if match_datadog_query(., "@severity: \"*\"") {
.custom.redis.severity = "notice"
} else if match_datadog_query(., "@severity: \"#\"") {
.custom.redis.severity = "warning"
}
'''

[[transforms.processing.logs.pipelines.redis.transforms]]
type = "remap"
source = '''
status = string(.custom.redis.severity) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[transforms.processing.logs.pipelines.consul]
name = "consul"
filter.type = "datadog_search"
filter.source = "source:consul"

[[transforms.processing.logs.pipelines.consul.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["==>\\s+(?>%{_status}:|)%{data:message}","(\\s*|\\t*)%{_date_2} %{_hostname} %{_app}\\[%{_thread_id}]:\\s+%{data:message}","(\\s*|\\t*)(%{_date}|%{_date_3}) \\[%{_status}\\] %{_app}:( %{_event}:)?%{data:message}"],
aliases: {
"Upstart_format": "==>\\s+(?>%{_status}:|)%{data:message}",
"Consul_template": "(\\s*|\\t*)%{_date_2} %{_hostname} %{_app}\\[%{_thread_id}]:\\s+%{data:message}",
"Default_format": "(\\s*|\\t*)(%{_date}|%{_date_3}) \\[%{_status}\\] %{_app}:( %{_event}:)?%{data:message}",
"_date": "%{date(\"yyyy/MM/dd HH:mm:ss\"):timestamp}",
"_date_2": "%{date(\"MMM dd HH:mm:ss\"):timestamp}",
"_date_3": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZZ\"):timestamp}",
"_status": "%{word:level}",
"_app": "%{data:app}",
"_event": "%{word:event}",
"_hostname": "%{notSpace:hostname}",
"_thread_id": "%{integer:logger.thread.id}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.consul.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.consul.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[transforms.processing.logs.pipelines.python]
name = "python"
filter.type = "datadog_search"
filter.source = "source:python"

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["(%{_python_prefix}|%{_datadog_prefix})\\s+%{data}Traceback \\(most recent call last\\):\\s*%{data:error.stack}","(%{_python_prefix}|%{_datadog_prefix})\\s+%{data}","%{date(\"yyyy-MM-dd'T'HH:mm:ss','SSS\"):timestamp}\\s+%{word:levelname}\\s+%{data}((\\n|\\t)%{data:error.stack})?"],
aliases: {
"traceback_format": "(%{_python_prefix}|%{_datadog_prefix})\\s+%{data}Traceback \\(most recent call last\\):\\s*%{data:error.stack}",
"python_format": "(%{_python_prefix}|%{_datadog_prefix})\\s+%{data}",
"python_fallback": "%{date(\"yyyy-MM-dd'T'HH:mm:ss','SSS\"):timestamp}\\s+%{word:levelname}\\s+%{data}((\\n|\\t)%{data:error.stack})?",
"_datadog_prefix": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):timestamp} %{word:levelname}\\s+\\[%{notSpace:process.name}\\]\\s+\\[%{notSpace:filename}:%{number:lineno}\\]\\s+\\[%{data}dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\] -",
"_python_prefix": "%{date(\"yyyy-MM-dd'T'HH:mm:ss','SSS\"):timestamp}\\s+%{word:levelname}\\s+\\[%{notSpace:process.name}\\]\\s+\\[%{integer:process.id}\\]"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
status = string(.custom.levelname) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
if exists(.custom.traceback) {
.custom.error.stack = .custom.traceback
}
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
if exists(.custom.name) {
.custom.logger.name = .custom.name
}
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
if exists(.custom.threadName) {
.custom.logger.thread_name = .custom.threadName
}
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .error.stack,
patterns: ["File \"%{notSpace:filename}\", line %{integer:lineno}.*\\s+%{regex(\"[a-zA-Z]*Error[a-zA-Z]*\"):error.kind}: %{data:error.message}"],
aliases: {
"parsing_traceback": "File \"%{notSpace:filename}\", line %{integer:lineno}.*\\s+%{regex(\"[a-zA-Z]*Error[a-zA-Z]*\"):error.kind}: %{data:error.message}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.trace_id) {
.trace_id = .custom.dd.trace_id
}
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.env) {
.custom.env = .custom.dd.env
}
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.version) {
.custom.version = .custom.dd.version
}
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
.service = .custom.dd.service
}
'''

[transforms.processing.logs.pipelines.rabbitmq]
name = "rabbitmq"
filter.type = "datadog_search"
filter.source = "source:rabbitmq"

[[transforms.processing.logs.pipelines.rabbitmq.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["=%{_status} REPORT(====)? (%{_date_d}|%{_date_dd}) =(==)?\\s*%{data:message}","%{_date_latest}\\s\\[%{_status}\\]\\s*%{data:message}"],
aliases: {
"rabbit_default": "=%{_status} REPORT(====)? (%{_date_d}|%{_date_dd}) =(==)?\\s*%{data:message}",
"rabbit_latest": "%{_date_latest}\\s\\[%{_status}\\]\\s*%{data:message}",
"_date_d": "%{date(\"d-MMM-yyyy::HH:mm:ss\"):timestamp}",
"_date_dd": "%{date(\"dd-MMM-yyyy::HH:mm:ss\"):timestamp}",
"_date_latest": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):timestamp}",
"_status": "%{word:status}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.rabbitmq.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.rabbitmq.transforms]]
type = "remap"
source = '''
status = string(.custom.status) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.rabbitmq.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
.message = .custom.message
}
'''

[transforms.processing.logs.pipelines.zookeeper]
name = "zookeeper"
filter.type = "datadog_search"
filter.source = "source:zookeeper"

[[transforms.processing.logs.pipelines.zookeeper.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["(%{_date_ms}|%{_duration})\\s+\\[%{_thread_name}\\]\\s+%{_status}\\s+%{_logger_name}\\s*(%{_context}\\s*)?- %{data:msg}((\\n|\\t)%{data:error.stack})?","%{_date} %{_status}\\s+%{_logger_name}:%{_line}\\s+- %{data:msg}((\\n|\\t)%{data:error.stack})?"],
aliases: {
"Zookeeper_default": "(%{_date_ms}|%{_duration})\\s+\\[%{_thread_name}\\]\\s+%{_status}\\s+%{_logger_name}\\s*(%{_context}\\s*)?- %{data:msg}((\\n|\\t)%{data:error.stack})?",
"Zookeeper_recommended": "%{_date} %{_status}\\s+%{_logger_name}:%{_line}\\s+- %{data:msg}((\\n|\\t)%{data:error.stack})?",
"_date": "%{date(\"yyyy-MM-dd HH:mm:ss\"):timestamp}",
"_date_ms": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):timestamp}",
"_duration": "%{integer:duration}",
"_thread_name": "%{notSpace:logger.thread_name}",
"_status": "%{word:status}",
"_logger_name": "%{notSpace:logger.name}",
"_context": "%{notSpace:logger.context}",
"_line": "%{integer:line}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.zookeeper.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .error.stack,
patterns: ["%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"],
aliases: {
"error_rule": "%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.zookeeper.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.zookeeper.transforms]]
type = "remap"
source = '''
status = string(.custom.status) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.zookeeper.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.elasticsearch]
name = "elasticsearch"
filter.type = "datadog_search"
filter.source = "source:elasticsearch"

[[transforms.processing.logs.pipelines.elasticsearch.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[index.search.slowlog.%{_operation}\\] (\\[%{_node}\\] )?\\[%{_index}\\]\\[%{_shard}\\] took\\[.*\\], took_millis\\[%{_duration}\\].*","\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[index.indexing.slowlog.%{_operation}\\] (\\[%{_node}\\] )?\\[%{_index}\\] took\\[.*\\], took_millis\\[%{_duration}\\].*","\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[%{_logger}\\s*\\]\\s*(\\[%{_node}\\])?.*"],
aliases: {
"Elasticsearch_search_query": "\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[index.search.slowlog.%{_operation}\\] (\\[%{_node}\\] )?\\[%{_index}\\]\\[%{_shard}\\] took\\[.*\\], took_millis\\[%{_duration}\\].*",
"Elasticsearch_slow_indexing": "\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[index.indexing.slowlog.%{_operation}\\] (\\[%{_node}\\] )?\\[%{_index}\\] took\\[.*\\], took_millis\\[%{_duration}\\].*",
"Elasticsearch_default": "\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[%{_logger}\\s*\\]\\s*(\\[%{_node}\\])?.*",
"_date": "%{date(\"yyyy-MM-dd'T'HH:mm:ss,SSS\"):timestamp}",
"_date_format2": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):timestamp}",
"_status": "%{word:level}",
"_operation": "%{notSpace:elasticsearch.operation}",
"_node": "%{hostname:nodeId}",
"_index": "%{notSpace:elasticsearch.index}",
"_shard": "%{integer:elasticsearch.shard}",
"_duration": "%{integer:duration:scale(1000000)}",
"_logger": "%{notSpace:logger.name}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.elasticsearch.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.elasticsearch.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.elasticsearch.transforms]]
type = "remap"
source = '''
if exists(.custom.nodeId) {
.custom.node_name = .custom.nodeId
}
'''

[transforms.processing.logs.pipelines.kafka]
name = "kafka"
filter.type = "datadog_search"
filter.source = "source:kafka"

[[transforms.processing.logs.pipelines.kafka.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["(%{_date_ms}|%{_duration})\\s+\\[%{_thread_name}\\]\\s+%{_status}\\s+%{_logger_name}\\s*(%{_context}\\s*)?- %{data:msg}((\\n|\\t)%{data:error.stack})?","%{_date} %{_status}\\s+%{_logger_name}:%{_line}\\s+- %{data:msg}((\\n|\\t)%{data:error.stack})?","\\[%{_date_ms}\\] %{_status} %{data:msg} \\(%{_logger_name}\\)"],
aliases: {
"Kafka_default": "(%{_date_ms}|%{_duration})\\s+\\[%{_thread_name}\\]\\s+%{_status}\\s+%{_logger_name}\\s*(%{_context}\\s*)?- %{data:msg}((\\n|\\t)%{data:error.stack})?",
"Kafka_recommended": "%{_date} %{_status}\\s+%{_logger_name}:%{_line}\\s+- %{data:msg}((\\n|\\t)%{data:error.stack})?",
"Kafka_standard": "\\[%{_date_ms}\\] %{_status} %{data:msg} \\(%{_logger_name}\\)",
"_date": "%{date(\"yyyy-MM-dd HH:mm:ss\"):timestamp}",
"_date_ms": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):timestamp}",
"_duration": "%{integer:duration}",
"_thread_name": "%{notSpace:logger.thread_name}",
"_status": "%{word:status}",
"_logger_name": "%{notSpace:logger.name}",
"_context": "%{notSpace:logger.context}",
"_line": "%{integer:line}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kafka.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .error.stack,
patterns: ["%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"],
aliases: {
"error_rule": "%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kafka.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.kafka.transforms]]
type = "remap"
source = '''
status = string(.custom.status) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.kafka.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.couchdb]
name = "couchdb"
filter.type = "datadog_search"
filter.source = "source:couchdb"

[[transforms.processing.logs.pipelines.couchdb.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["\\[%{_date_access}\\] \\[%{_level}\\] %{notSpace} %{_client_ip} - - %{_method} %{_url} %{_status_code}.*","\\[%{_level}\\] %{_date} %{_user}@%{_client_ip} %{notSpace} (-------- CRASH REPORT\\s*%{data:error.stack}|%{data})","%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date_access} \\[%{_level}\\] .*"],
aliases: {
"http_rule": "\\[%{_date_access}\\] \\[%{_level}\\] %{notSpace} %{_client_ip} - - %{_method} %{_url} %{_status_code}.*",
"default_format": "\\[%{_level}\\] %{_date} %{_user}@%{_client_ip} %{notSpace} (-------- CRASH REPORT\\s*%{data:error.stack}|%{data})",
"fallback_format": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date_access} \\[%{_level}\\] .*",
"_date_access": "(%{date(\"EEE, dd MMM yyyy HH:mm:ss z\"):date_access}|%{date(\"EEE, d MMM yyyy HH:mm:ss z\"):date_access})",
"_date": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ\"):date_access}",
"_level": "%{word:level}",
"_method": "%{word:http.method}",
"_url": "%{notSpace:http.url}",
"_status_code": "%{number:http.status_code}",
"_client_ip": "%{notSpace:network.client.ip}",
"_user": "%{notSpace:db.user}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.couchdb.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
.timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.pipelines.couchdb.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.couchdb.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.http.url); err == null) {
  .custom.http.url_details = details
}
'''

[transforms.processing.logs.pipelines.docker]
name = "docker"
filter.type = "datadog_search"
filter.source = "source:docker"

[[transforms.processing.logs.pipelines.docker.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{_level}%{_kube_date}\\s+%{_thread_id} %{_logger_name}:%{_line}\\].*","%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-) \"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*","(?>\\[ %{word:process.name} \\] )?%{_dd_date} \\| %{word:level} \\| \\(%{_logger_name}:%{_line} in %{word:datadog.process}\\) \\| .*"],
aliases: {
"Kubernetes_format": "%{_level}%{_kube_date}\\s+%{_thread_id} %{_logger_name}:%{_line}\\].*",
"access.combined": "%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-) \"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*",
"datadog_format": "(?>\\[ %{word:process.name} \\] )?%{_dd_date} \\| %{word:level} \\| \\(%{_logger_name}:%{_line} in %{word:datadog.process}\\) \\| .*",
"_auth": "%{notSpace:http.auth:nullIf(\"-\")}",
"_bytes_written": "%{integer:network.bytes_written}",
"_client_ip": "%{ipOrHost:network.client.ip}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
"_url": "%{notSpace:http.url}",
"_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
"_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
"_referer": "%{notSpace:http.referer}",
"_status_code": "%{integer:http.status_code}",
"_method": "%{word:http.method}",
"_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):timestamp}",
"_dd_date": "%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp}",
"_x_forwarded_for": "%{regex(\"[^\\\\\\\"]*\"):http._x_forwarded_for:nullIf(\"-\")}",
"_level": "%{regex(\"[\\\\w]\"):level}",
"_kube_date": "%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}",
"_thread_id": "%{number:logger.thread_id}",
"_logger_name": "%{notSpace:logger.name}",
"_line": "%{number:lineno}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.docker.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.docker.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[transforms.processing.logs.pipelines.datadog_agent]
name = "datadog_agent"
filter.type = "datadog_search"
filter.source = "source:(agent OR datadog-agent OR datadog-agent-cluster-worker OR datadog-cluster-agent OR process-agent OR security-agent OR system-probe OR trace-agent)"

[[transforms.processing.logs.pipelines.datadog_agent.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["        %{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{notSpace:agent} \\| %{word:level} \\| \\(%{notSpace:filename}:%{number:lineno} in %{word:process}\\) \\|( %{data} \\|)?( - \\|)?( \\(%{notSpace:pyFilename}:%{number:pyLineno}\\) \\|)?%{data}","%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{word:level} \\| \\(%{notSpace:filename}:%{number:lineno} in %{word:process}\\)%{data}","     %{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{notSpace:agent} \\| %{word:level}\\s+\\| %{word:class} \\| %{data}"],
aliases: {
"agent_rule": "        %{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{notSpace:agent} \\| %{word:level} \\| \\(%{notSpace:filename}:%{number:lineno} in %{word:process}\\) \\|( %{data} \\|)?( - \\|)?( \\(%{notSpace:pyFilename}:%{number:pyLineno}\\) \\|)?%{data}",
"agent_rule_pre_611": "%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{word:level} \\| \\(%{notSpace:filename}:%{number:lineno} in %{word:process}\\)%{data}",
"jmxfetch_rule": "     %{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{notSpace:agent} \\| %{word:level}\\s+\\| %{word:class} \\| %{data}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.datadog_agent.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.datadog_agent.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[transforms.processing.logs.pipelines.ruby]
name = "ruby"
filter.type = "datadog_search"
filter.source = "source:ruby"

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.logger) {
.custom.logger.name = .custom.logger
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.thread) {
.custom.logger.thread_name = .custom.thread
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.exception) {
.custom.error.stack = .custom.exception
} else if exists(.custom.stack_trace) {
.custom.error.stack = .custom.stack_trace
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} %{integer:http.status_code}(?: (?:%{word}|Internal Server Error))? in %{number:duration}ms \\((?>Views: %{number:views}ms \\| )?ActiveRecord: %{number:activerecord}ms\\)","(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} by %{notSpace:processor}.*","(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} %{word:http.method} \"%{notSpace:http.url_details.path}\" for %{ipOrHost:network.client.ip}.*","%{_datadog_prefix} %{data}(?>(\\n|\\t)%{data:error.stack})?","(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?Received: %{data::json}","%{_ruby_log_prefix}\\s+%{data:error.message}(?>(\\n|\\t)%{data:error.stack})?","%{data}"],
aliases: {
"completed_rule": "(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} %{integer:http.status_code}(?: (?:%{word}|Internal Server Error))? in %{number:duration}ms \\((?>Views: %{number:views}ms \\| )?ActiveRecord: %{number:activerecord}ms\\)",
"processing_rule": "(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} by %{notSpace:processor}.*",
"started_rule": "(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} %{word:http.method} \"%{notSpace:http.url_details.path}\" for %{ipOrHost:network.client.ip}.*",
"datadog_format": "%{_datadog_prefix} %{data}(?>(\\n|\\t)%{data:error.stack})?",
"received_rule": "(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?Received: %{data::json}",
"Ruby_default": "%{_ruby_log_prefix}\\s+%{data:error.message}(?>(\\n|\\t)%{data:error.stack})?",
"Ruby_keyvalue": "%{data}",
"_date": "(?:%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\"):date}|%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS\"):date})",
"_status": "%{word:level}",
"_thread_id": "%{word:logger.thread_id}",
"_thread_name": "%{notSpace:logger.thread_name}",
"_logger_name": "%{notSpace:logger.name}",
"_ruby_log_prefix": "%{word}, \\[%{_date} #%{_thread_id}\\]\\s+%{_status}\\s+--\\s+(?:%{_logger_name})?:(?:\\s+%{_trace_rule})?",
"_trace_rule": "\\[%{data}dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]",
"_datadog_prefix": "\\[%{date(\"yyyy-MM-dd HH:mm:ss Z\"):date}\\]\\[%{word:application}\\]\\[%{_status}\\]%{_trace_rule}",
"_activate_status": "%{word:active_directory.process_status}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
.timestamp = .custom.date
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .error.stack,
patterns: ["%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"],
aliases: {
"error_rule": "%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.method) {
.custom.http.method = .custom.method
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.path) {
.custom.http.url_details.path = .custom.path
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.status) {
.custom.http.status_code = .custom.status
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.trace_id) {
.trace_id = .custom.dd.trace_id
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.env) {
.custom.env = .custom.dd.env
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.version) {
.custom.version = .custom.dd.version
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
.service = .custom.dd.service
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if (result, err = .custom.duration*1000000; err == null) {
  .custom.duration = result
}
'''

[transforms.processing.logs.pipelines.vault]
name = "vault"
filter.type = "datadog_search"
filter.source = "source:vault"

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{_date}\\s+\\[%{_level}\\]\\s+%{notSpace:vault.service}:\\s+%{data:message}","%{_date}\\s+\\[%{_level}\\]\\s+%{data:message}"],
aliases: {
"vault_server_svc": "%{_date}\\s+\\[%{_level}\\]\\s+%{notSpace:vault.service}:\\s+%{data:message}",
"vault_server": "%{_date}\\s+\\[%{_level}\\]\\s+%{data:message}",
"_date": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):timestamp}",
"_level": "%{word:level}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
.message = .custom.message
}
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
if exists(.custom.auth.display_name) {
.custom.usr.id = .custom.auth.display_name
} else if exists(.custom.auth.metatdata.username) {
.custom.usr.id = .custom.auth.metatdata.username
}
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
if exists(.custom.request.path) {
.custom.http.url_details.path = .custom.request.path
}
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
if exists(.custom.request.data.http_status_code) {
.custom.http.status_code = .custom.request.data.http_status_code
}
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
if exists(.custom.request.remote_address) {
.custom.network.client.ip = .custom.request.remote_address
}
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''

#TODO: geo-ip-parser
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
if exists(.custom.request.operation) {
.custom.http.method = .custom.request.operation
}
'''

[transforms.processing.logs.pipelines.nginx_ingress_controller]
name = "nginx_ingress_controller"
filter.type = "datadog_search"
filter.source = "source:nginx-ingress-controller"

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{_client_ip}(?: - \\[%{notSpace}\\])? - %{_ident} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-) \"%{_referer}\" \"%{_user_agent}\" %{_request_size} %{_duration} \\[%{_proxy_name}\\](?: \\[%{_alternate_proxy_name}?\\])? (?:%{_upstream_ip}:%{_upstream_port}|-)(?:, %{notSpace})?(?:, %{notSpace})? (?:%{_bytes_read}|-)(?:, %{number}|, -)?(?:, %{number}|, -)? (?:%{_upstream_time}|-)(?:, %{number}|, -)?(?:, %{number}|, -)? (?:%{_upstream_status}|-)(?:, %{number}|, -)?(?:, %{number}|, -)?(?: %{_request_id})?.*","%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, %{data})?","%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):date_access}\\s+%{number} %{notSpace:logger.name}:%{number:lineno}\\] .*"],
aliases: {
"access.common": "%{_client_ip}(?: - \\[%{notSpace}\\])? - %{_ident} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-) \"%{_referer}\" \"%{_user_agent}\" %{_request_size} %{_duration} \\[%{_proxy_name}\\](?: \\[%{_alternate_proxy_name}?\\])? (?:%{_upstream_ip}:%{_upstream_port}|-)(?:, %{notSpace})?(?:, %{notSpace})? (?:%{_bytes_read}|-)(?:, %{number}|, -)?(?:, %{number}|, -)? (?:%{_upstream_time}|-)(?:, %{number}|, -)?(?:, %{number}|, -)? (?:%{_upstream_status}|-)(?:, %{number}|, -)?(?:, %{number}|, -)?(?: %{_request_id})?.*",
"error.format": "%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, %{data})?",
"controller_format": "%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):date_access}\\s+%{number} %{notSpace:logger.name}:%{number:lineno}\\] .*",
"_request_id": "%{notSpace:http.request_id}",
"_upstream_status": "%{number:http.upstream_status_code}",
"_upstream_time": "%{number:http.upstream_duration}",
"_bytes_read": "%{number:network.bytes_read}",
"_upstream_port": "%{number:network.destination.port}",
"_upstream_ip": "%{ipOrHost:network.destination.ip}",
"_proxy_name": "%{notSpace:proxy.name}",
"_alternate_proxy_name": "%{notSpace:proxy.alternate_name}",
"_duration": "%{number:duration:scale(1000000000)}",
"_request_size": "%{number:network.request_size}",
"_bytes_written": "%{integer:network.bytes_written}",
"_client_ip": "%{ipOrHost:network.client.ip}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
"_url": "%{notSpace:http.url}",
"_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
"_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
"_referer": "%{notSpace:http.referer}",
"_status_code": "%{integer:http.status_code}",
"_method": "%{word:http.method}",
"_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}",
"_x_forwarded_for": "%{regex(\"[^\\\\\\\"]*\"):http._x_forwarded_for:nullIf(\"-\")}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
if exists(.custom.client) {
.custom.network.client.ip = .custom.client
}
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .request,
patterns: ["(?>%{_method} |)%{_url}(?> %{_version}|)"],
aliases: {
"request_parsing": "(?>%{_method} |)%{_url}(?> %{_version}|)",
"_method": "%{word:http.method}",
"_url": "%{notSpace:http.url}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.http.url); err == null) {
  .custom.http.url_details = details
}
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
if (details, err = parse_user_agent(.custom.http.useragent); err == null) {
  .custom.http.useragent_details = details
}
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
.timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@http.status_code:[200 TO 299]") {
.custom.http.status_category = "OK"
} else if match_datadog_query(., "@http.status_code:[300 TO 399]") {
.custom.http.status_category = "notice"
} else if match_datadog_query(., "@http.status_code:[400 TO 499]") {
.custom.http.status_category = "warning"
} else if match_datadog_query(., "@http.status_code:[500 TO 599]") {
.custom.http.status_category = "error"
}
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[transforms.processing.logs.pipelines.mysql]
name = "mysql"
filter.type = "datadog_search"
filter.source = "source:mysql"

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["(%{_timestamp}\\s+|(?:%{_rawtimestamp}\\s+)?)%{integer:thread_id}\\s+%{_operation}\\s+%{_raw_query}","(%{_timestamp}|%{_timestamp_mariadb_post_1015}) %{integer:thread_id} \\[%{_severity}\\] %{data:message}","%{_rawtimestamp}\\s+(?>InnoDB:|\\[%{_severity}\\])\\s+%{data:message}","(%{_time_line}\\s)?(%{_user_line}\\s)?\\# %{data}(%{_instance_line}\\s)?%{_set_line}\\s%{_query_line}"],
aliases: {
"query_format": "(%{_timestamp}\\s+|(?:%{_rawtimestamp}\\s+)?)%{integer:thread_id}\\s+%{_operation}\\s+%{_raw_query}",
"default_format": "(%{_timestamp}|%{_timestamp_mariadb_post_1015}) %{integer:thread_id} \\[%{_severity}\\] %{data:message}",
"raw_default_format": "%{_rawtimestamp}\\s+(?>InnoDB:|\\[%{_severity}\\])\\s+%{data:message}",
"slow_query_format": "(%{_time_line}\\s)?(%{_user_line}\\s)?\\# %{data}(%{_instance_line}\\s)?%{_set_line}\\s%{_query_line}",
"_timestamp": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ\"):db.date}",
"_timestamp_mariadb_post_1015": "(%{date(\"yyyy-MM-dd HH:mm:ss\"):db.date}|%{date(\"yyyy-MM-dd  H:mm:ss\"):db.date})",
"_rawtimestamp": "%{date(\"yyMMdd HH:mm:ss\"):db.date}",
"_severity": "%{notSpace:db.severity}",
"_client_ip": "%{ipOrHost:network.client.ip}",
"_client_port": "%{integer:network.client.port}",
"_operation": "%{word:db.operation}",
"_database": "%{word:db.instance}",
"_raw_query": "%{data:db.statement}",
"_time_line": "\\# Time: %{notSpace}(\\s+%{notSpace})?",
"_user_line": "\\# User@Host\\: %{notSpace:db.user}\\s+@\\s+%{notSpace:db.host}\\s+\\[(%{notSpace:network.client.ip})?\\](\\s+Id\\:\\s+%{number:mysql.query.id})?",
"_instance_line": "use %{notSpace:db.instance};",
"_set_line": "SET timestamp=%{number:mysql.query.timestamp};",
"_query_line": "%{data:db.slow_statement}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .db.slow_statement,
patterns: ["%{word:db.operation} .*"],
aliases: {
"slow_query_format": "%{word:db.operation} .*"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
if exists(.custom.db.slow_statement) {
.custom.db.statement = .custom.db.slow_statement
}
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
if (result, err = .custom.Query_time * 1000000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
if (result, err = .custom.mysql.query.timestamp * 1000; err == null) {
  .custom.db.date = result
}
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
if exists(.custom.Bytes_sent) {
.custom.network.bytes_written = .custom.Bytes_sent
}
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
if exists(.custom.Bytes_received) {
.custom.network.bytes_read = .custom.Bytes_received
}
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
if exists(.custom.db.date) {
.timestamp = .custom.db.date
}
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
status = string(.custom.db.severity) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[transforms.processing.logs.pipelines.kubernetes_cluster_autoscaler]
name = "kubernetes_cluster_autoscaler"
filter.type = "datadog_search"
filter.source = "source:cluster-autoscaler"

[[transforms.processing.logs.pipelines.kubernetes_cluster_autoscaler.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"],
aliases: {
"cluster_scheduler": "%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kubernetes_cluster_autoscaler.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.kubernetes_cluster_autoscaler.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.kubernetes_cluster_autoscaler.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.aws_alb_ingress_controller]
name = "aws_alb_ingress_controller"
filter.type = "datadog_search"
filter.source = "source:aws-alb-ingress-controller"

[[transforms.processing.logs.pipelines.aws_alb_ingress_controller.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"],
aliases: {
"aws_alb_ingress_controller": "%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.aws_alb_ingress_controller.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.aws_alb_ingress_controller.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.aws_alb_ingress_controller.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.proxysql]
name = "proxysql"
filter.type = "datadog_search"
filter.source = "source:proxysql"

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{_longDate}\\s+%{_logger}\\:\\s+%{_level}\\s+%{data:message}","%{_longDate}\\s+%{_level}\\s+%{data:message}","%{_longDate}.*%{_level}%{data:message}","%{data:proxysql_service}\\s+%{_revision}\\s+--\\s+%{notSpace:logger.name}\\s+--\\s+%{_textDate}"],
aliases: {
"extendedFormat": "%{_longDate}\\s+%{_logger}\\:\\s+%{_level}\\s+%{data:message}",
"simplifiedFormat": "%{_longDate}\\s+%{_level}\\s+%{data:message}",
"safeGuard": "%{_longDate}.*%{_level}%{data:message}",
"stdout": "%{data:proxysql_service}\\s+%{_revision}\\s+--\\s+%{notSpace:logger.name}\\s+--\\s+%{_textDate}",
"_longDate": "%{date(\"yyyy-MM-dd HH:mm:ss\"):date}",
"_textDate": "%{date(\"EEE MMM dd HH:mm:ss yyyy\"):date}",
"_level": "\\[%{word:level}\\]",
"_logger": "%{notSpace:logger.name}\\:%{number:logger.line_number}\\:%{notSpace:logger.method_name}\\(.*\\)",
"_revision": "rev.\\s+%{regex(\"\\\\d+.\\\\d+.\\\\d+\"):revision}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .duration,
patterns: ["%{number:duration:scale(1000000)}ms","%{number:duration:scale(1000000000)}s","%{number:duration:scale(1000)}us","%{number:duration:scale(1)}ns"],
aliases: {
"duration_ms": "%{number:duration:scale(1000000)}ms",
"duration_s": "%{number:duration:scale(1000000000)}s",
"duration_us": "%{number:duration:scale(1000)}us",
"duration_ns": "%{number:duration:scale(1)}ns"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .client_addr,
patterns: ["%{ipOrHost:network.client.ip}:%{number:network.client.port}"],
aliases: {
"ip_and_port": "%{ipOrHost:network.client.ip}:%{number:network.client.port}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .proxy_addr,
patterns: ["%{ipOrHost:network.proxysql.ip}:%{number:network.proxysql.port}"],
aliases: {
"ip_and_port": "%{ipOrHost:network.proxysql.ip}:%{number:network.proxysql.port}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .extra_info,
patterns: ["%{notSpace:logger.name}\\:%{number:logger.line_number}\\:%{notSpace:logger.method_name}"],
aliases: {
"logger": "%{notSpace:logger.name}\\:%{number:logger.line_number}\\:%{notSpace:logger.method_name}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
if exists(.custom.schemaname) {
.custom.db.instance = .custom.schemaname
}
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
if exists(.custom.username) {
.custom.db.user = .custom.username
}
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
if exists(.custom.thread_id) {
.custom.logger.thread_id = .custom.thread_id
}
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .client,
patterns: ["%{ipOrHost:network.client.ip}:%{number:network.client.port}"],
aliases: {
"ip_and_port": "%{ipOrHost:network.client.ip}:%{number:network.client.port}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
if (result, err = .custom.duration_us * 1000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
if exists(.custom.query) {
.custom.db.statement = .custom.query
}
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
if exists(.custom.rows_affected) {
.custom.db.rows_affected = .custom.rows_affected
}
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
if exists(.custom.rows_sent) {
.custom.db.rows_sent = .custom.rows_sent
}
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .server,
patterns: ["%{ipOrHost:network.proxysql.ip}:%{number:network.proxysql.port}"],
aliases: {
"ip_and_port": "%{ipOrHost:network.proxysql.ip}:%{number:network.proxysql.port}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
.timestamp = .custom.date
} else if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
} else if exists(.custom.endtime_timestamp_us) {
.timestamp = .custom.endtime_timestamp_us
}
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
.message = .custom.message
}
'''

[transforms.processing.logs.pipelines.azure]
name = "azure"
filter.type = "datadog_search"
filter.source = "source:(azure OR azure.alertsmanagement OR azure.analysisservices OR azure.apiconfiguration OR azure.apimanagement OR azure.authorization OR azure.automation OR azure.batchai OR azure.batchazure.cache OR azure.blockchain OR azure.cache OR azure.cdn OR azure.classiccompute OR azure.classicstorage OR azure.cognitiveservices OR azure.containerinstance OR azure.containerregistry OR azure.containerservice OR azure.datafactory OR azure.datalakestore OR azure.dbformariadb OR azure.dbformysql OR azure.dbforpostgresql OR azure.devices OR azure.documentdb OR azure.enterpriseknowledgegraph OR azure.eventgrid OR azure.eventhub OR azure.hdinsight OR azure.insights OR azure.iotcentral OR azure.keyvault OR azure.kusto OR azure.logic OR azure.machinelearningservices OR azure.managedidentity OR azure.operationalinsights OR azure.operationsmanagement OR azure.peering OR azure.relay OR azure.resourcegroup OR azure.resources OR azure.search OR azure.security OR azure.servicebus OR azure.servicefabric OR azure.streamanalytics OR azure.subscription OR azure.synapse)"

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
if exists(.custom.category) {
.custom.evt.category = .custom.category
}
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
if exists(.custom.operationName) {
.custom.evt.name = .custom.operationName
}
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
if exists(.custom.resultType) {
.custom.evt.outcome = .custom.resultType
}
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
if exists(.custom.callerIpAddress) {
.custom.network.client.ip = .custom.callerIpAddress
}
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
if (result, err = .custom.durationMs * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
if exists(.custom.identity.authorization.evidence.principalId) {
.custom.usr.id = .custom.identity.authorization.evidence.principalId
}
'''

[transforms.processing.logs.pipelines.azure_web]
name = "azure_web"
filter.type = "datadog_search"
filter.source = "source:azure.web"

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
if exists(.custom.category) {
.custom.evt.category = .custom.category
}
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
if exists(.custom.operationName) {
.custom.evt.name = .custom.operationName
}
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
if exists(.custom.resultType) {
.custom.evt.outcome = .custom.resultType
}
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
if exists(.custom.callerIpAddress) {
.custom.network.client.ip = .custom.callerIpAddress
}
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
if (result, err = .custom.durationMs * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
if exists(.custom.identity.authorization.evidence.principalId) {
.custom.usr.id = .custom.identity.authorization.evidence.principalId
}
'''

[transforms.processing.logs.pipelines.azure_storage]
name = "azure_storage"
filter.type = "datadog_search"
filter.source = "source:azure.storage"

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
if exists(.custom.category) {
.custom.evt.category = .custom.category
}
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
if exists(.custom.operationName) {
.custom.evt.name = .custom.operationName
}
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
if exists(.custom.resultType) {
.custom.evt.outcome = .custom.resultType
}
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
if exists(.custom.callerIpAddress) {
.custom.network.client.ip = .custom.callerIpAddress
}
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
if (result, err = .custom.durationMs * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
if exists(.custom.identity.authorization.evidence.principalId) {
.custom.usr.id = .custom.identity.authorization.evidence.principalId
}
'''

[transforms.processing.logs.pipelines.azure_network]
name = "azure_network"
filter.type = "datadog_search"
filter.source = "source:azure.network"

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
if exists(.custom.properties.eventProperties.title) {
.message = .custom.properties.eventProperties.title
}
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
if exists(.custom.category) {
.custom.evt.category = .custom.category
}
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
if exists(.custom.operationName) {
.custom.evt.name = .custom.operationName
}
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
if exists(.custom.resultType) {
.custom.evt.outcome = .custom.resultType
}
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
if exists(.custom.callerIpAddress) {
.custom.network.client.ip = .custom.callerIpAddress
}
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
if (result, err = .custom.durationMs * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
if exists(.custom.identity.authorization.evidence.principalId) {
.custom.usr.id = .custom.identity.authorization.evidence.principalId
}
'''

[transforms.processing.logs.pipelines.azure_compute]
name = "azure_compute"
filter.type = "datadog_search"
filter.source = "source:azure.compute"

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
if exists(.custom.properties.eventProperties.title) {
.message = .custom.properties.eventProperties.title
}
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
if exists(.custom.category) {
.custom.evt.category = .custom.category
}
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
if exists(.custom.operationName) {
.custom.evt.name = .custom.operationName
}
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
if exists(.custom.resultType) {
.custom.evt.outcome = .custom.resultType
}
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
if exists(.custom.callerIpAddress) {
.custom.network.client.ip = .custom.callerIpAddress
}
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
if (result, err = .custom.durationMs * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
if exists(.custom.identity.authorization.evidence.principalId) {
.custom.usr.id = .custom.identity.authorization.evidence.principalId
}
'''

[transforms.processing.logs.pipelines.etcd]
name = "etcd"
filter.type = "datadog_search"
filter.source = "source:etcd"

[[transforms.processing.logs.pipelines.etcd.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{_date} %{word: level} \\| (%{notSpace:package}: )?%{data:message}"],
aliases: {
"etcd": "%{_date} %{word: level} \\| (%{notSpace:package}: )?%{data:message}",
"_date": "%{date(\"yyyy-MM-dd HH:mm:ss.SSSSSS\"):date}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.etcd.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
.timestamp = .custom.date
}
'''

[[transforms.processing.logs.pipelines.etcd.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
.message = .custom.message
}
'''

[[transforms.processing.logs.pipelines.etcd.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[transforms.processing.logs.pipelines.glog_pipeline]
name = "glog_pipeline"
filter.type = "datadog_search"
filter.source = "source:(admission-webhook OR api-server OR cert-manager-acmesolver OR cert-manager-cainjector OR cert-manager-controller OR cert-manager-webhook OR cluster-proportional-autoscaler-amd64 OR hyperkube OR ip-masq-agent OR k8s-prometheus-adapter-amd64 OR kube-apiserver OR kube-controller-manager OR kube-proxy OR kube-state-metrics OR metacontroller OR metrics-server-amd64 OR prometheus-operator OR vpa-admission-controller OR vpa-recommender OR vpa-updater)"

[[transforms.processing.logs.pipelines.glog_pipeline.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{regex(\"[IWEF]\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"],
aliases: {
"glog_rule": "%{regex(\"[IWEF]\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.glog_pipeline.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.glog_pipeline.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.glog_pipeline.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.auth0]
name = "auth0"
filter.type = "datadog_search"
filter.source = "source:auth0"

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
if exists(.custom.data.date) {
.timestamp = .custom.data.date
}
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
if exists(.custom.data.ip) {
.custom.network.client.ip = .custom.data.ip
}
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''

#TODO: geo-ip-parser
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
if exists(.custom.data.user_agent) {
.custom.http.useragent = .custom.data.user_agent
}
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
if (details, err = parse_user_agent(.custom.http.useragent); err == null) {
  .custom.http.useragent_details = details
}
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
if exists(.custom.data.user_name) {
.custom.usr.id = .custom.data.user_name
}
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
if exists(.custom.data.user_name) {
.custom.usr.name = .custom.data.user_name
}
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
if exists(.custom.data.details.request.auth.user.email) {
.custom.usr.email = .custom.data.details.request.auth.user.email
}
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''

#TODO: lookup-processor
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''

#TODO: lookup-processor
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
.message = .custom.message
} else if exists(.custom.data.description) {
.message = .custom.data.description
} else if exists(.custom.evt.name) {
.message = .custom.evt.name
}
'''

[transforms.processing.logs.pipelines.kube_scheduler__glog_]
name = "kube_scheduler__glog_"
filter.type = "datadog_search"
filter.source = "source:(kube_scheduler OR kube-scheduler)"

[[transforms.processing.logs.pipelines.kube_scheduler__glog_.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"],
aliases: {
"kube_scheduler": "%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kube_scheduler__glog_.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.kube_scheduler__glog_.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.kube_scheduler__glog_.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.aws_ecs_agent]
name = "aws_ecs_agent"
filter.type = "datadog_search"
filter.source = "source:amazon-ecs-agent"

[[transforms.processing.logs.pipelines.aws_ecs_agent.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@http.status_code:[200 TO 299]") {
.custom.http.status_category = "OK"
} else if match_datadog_query(., "@http.status_code:[300 TO 399]") {
.custom.http.status_category = "notice"
} else if match_datadog_query(., "@http.status_code:[400 TO 499]") {
.custom.http.status_category = "warning"
} else if match_datadog_query(., "@http.status_code:[500 TO 599]") {
.custom.http.status_category = "error"
}
'''

[[transforms.processing.logs.pipelines.aws_ecs_agent.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.aws_ecs_agent.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.aws_ecs_agent.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.nodejs]
name = "nodejs"
filter.type = "datadog_search"
filter.source = "source:nodejs"

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["(?>\\[%{data}dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]\\s*)?\\[%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS\"):time}\\]\\s+\\[%{word:level}\\] %{data}","%{word:http.method} %{notSpace:http.url} %{number:http.status_code} (-|%{number:network.bytes_written}) - (%{number:duration:scale(1000000)} ms|%{number:duration:scale(1000000000)} s)","%{ipOrHost:network.client.ip} %{notSpace:http.ident:nullIf(\"-\")} %{notSpace:http.auth:nullIf(\"-\")} \\[%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}\\] \"(?>%{word:http.method} |)%{notSpace:http.url}(?> HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}|)\" %{number:http.status_code} (?>%{number:network.bytes_written}|-) \"%{notSpace:http.referer}\" \"%{regex(\"[^\\\\\\\"]*\"):http.useragent}\".*","(?>\\[%{data}dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]\\s*)?%{data::json}"],
aliases: {
"log4js_format": "(?>\\[%{data}dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]\\s*)?\\[%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS\"):time}\\]\\s+\\[%{word:level}\\] %{data}",
"web_access_morgan": "%{word:http.method} %{notSpace:http.url} %{number:http.status_code} (-|%{number:network.bytes_written}) - (%{number:duration:scale(1000000)} ms|%{number:duration:scale(1000000000)} s)",
"morgan_combined": "%{ipOrHost:network.client.ip} %{notSpace:http.ident:nullIf(\"-\")} %{notSpace:http.auth:nullIf(\"-\")} \\[%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}\\] \"(?>%{word:http.method} |)%{notSpace:http.url}(?> HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}|)\" %{number:http.status_code} (?>%{number:network.bytes_written}|-) \"%{notSpace:http.referer}\" \"%{regex(\"[^\\\\\\\"]*\"):http.useragent}\".*",
"fallback": "(?>\\[%{data}dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]\\s*)?%{data::json}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@http.status_code:[200 TO 299]") {
.custom.http.status_category = "OK"
} else if match_datadog_query(., "@http.status_code:[300 TO 399]") {
.custom.http.status_category = "notice"
} else if match_datadog_query(., "@http.status_code:[400 TO 499]") {
.custom.http.status_category = "warning"
} else if match_datadog_query(., "@http.status_code:[500 TO 599]") {
.custom.http.status_category = "error"
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@level:10") {
.custom.bunyan_level = "trace"
} else if match_datadog_query(., "@level:20") {
.custom.bunyan_level = "debug"
} else if match_datadog_query(., "@level:30") {
.custom.bunyan_level = "info"
} else if match_datadog_query(., "@level:40") {
.custom.bunyan_level = "warning"
} else if match_datadog_query(., "@level:50") {
.custom.bunyan_level = "error"
} else if match_datadog_query(., "@level:60") {
.custom.bunyan_level = "fatal"
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.bunyan_level) ?? string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.trace_id) {
.trace_id = .custom.dd.trace_id
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.http.url); err == null) {
  .custom.http.url_details = details
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if (details, err = parse_user_agent(.custom.http.useragent); err == null) {
  .custom.http.useragent_details = details
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.env) {
.custom.env = .custom.dd.env
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.version) {
.custom.version = .custom.dd.version
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
.service = .custom.dd.service
}
'''

[transforms.processing.logs.pipelines.postgresql]
name = "postgresql"
filter.type = "datadog_search"
filter.source = "source:postgresql"

[[transforms.processing.logs.pipelines.postgresql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{_prefix} %{_severity}:\\s+duration:\\s+%{_duration}\\s+ms\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})","%{_prefix} %{_severity}:\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})","(%{_timestamp}|%{_timestamp_ms} \\[%{_proc_id}])\\s+%{_severity}:\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})"],
aliases: {
"suggested_format_with_duration": "%{_prefix} %{_severity}:\\s+duration:\\s+%{_duration}\\s+ms\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})",
"suggested_format": "%{_prefix} %{_severity}:\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})",
"default_format": "(%{_timestamp}|%{_timestamp_ms} \\[%{_proc_id}])\\s+%{_severity}:\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})",
"_timestamp": "%{date(\"yyyy-MM-dd HH:mm:ss z\"):db.date}",
"_timestamp_ms": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS z\"):db.date}",
"_database": "%{notSpace:db.instance}",
"_raw_query": "%{data:db.statement}",
"_duration": "%{numberExt:duration}",
"_severity": "%{notSpace:db.severity}",
"_user": "%{notSpace:db.user}",
"_client_ip": "%{notSpace:network.client.ip}",
"_proc_id": "%{notSpace:postgres.proc_id}",
"_session_id": "%{notSpace:postgres.session_id}",
"_app": "%{notSpace:postgres.appname}",
"_prefix": "%{_timestamp_ms} \\[%{_proc_id}\\] %{_database} %{_app} %{_user} %{_client_ip} %{_session_id}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.postgresql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .db.statement,
patterns: ["%{word:db.operation} .*"],
aliases: {
"extract_operation": "%{word:db.operation} .*"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.postgresql.transforms]]
type = "remap"
source = '''
if exists(.custom.db.date) {
.timestamp = .custom.db.date
}
'''

[[transforms.processing.logs.pipelines.postgresql.transforms]]
type = "remap"
source = '''
status = string(.custom.db.severity) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.postgresql.transforms]]
type = "remap"
source = '''
if exists(.custom.db.instance) {
.custom.db = .custom.db.instance
}
'''

[[transforms.processing.logs.pipelines.postgresql.transforms]]
type = "remap"
source = '''
if (result, err = .custom.duration * 1000000; err == null) {
  .custom.duration = result
}
'''

[transforms.processing.logs.pipelines.cassandra]
name = "cassandra"
filter.type = "datadog_search"
filter.source = "source:cassandra"

[[transforms.processing.logs.pipelines.cassandra.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{_prefix} %{regex(\"Compacting\"):db.operation}.* %{_keyspace}\\/%{_table}:%{data:partition_key} \\(%{_bytes} bytes\\)","%{_prefix} %{regex(\"Flushing\"):db.operation}.*\\(Keyspace='%{_keyspace}', ColumnFamily='%{_table}'\\) %{data}: %{_onheap_total}\\/%{_offheap_total}, live: %{_onheap_live}\\/%{_offheap_live}, flushing: %{_onheap_flush}\\/%{_offheap_flush}, this: %{_onheap_this}\\/%{_offheap_this}","%{_prefix} %{regex(\"Enqueuing\"):db.operation}.* of %{_keyspace}: %{_onheap_bytes}%{data} \\(%{_onheap_pct}%\\) on-heap, %{_offheap_bytes} \\(%{_offheap_pct}%\\).*","%{_prefix} %{regex(\"Writing\"):db.operation}.*-%{_keyspace}%{data}\\(%{number:cassandra.bytes:scale(1000000)}%{data}, %{integer:cassandra.ops} ops, %{_onheap_pct}%\\/%{_offheap_pct}.*","%{_prefix} Completed %{regex(\"flushing\"):db.operation} %{_sstable} \\(%{number:cassandra.bytes_kb}KiB\\) for commitlog %{data:commitlog}","%{_prefix}\\s+%{regex(\"Compacted\"):db.operation}.* to \\[%{_sstable}\\].\\s+%{notSpace:cassandra.bytes_in} bytes to %{notSpace:cassandra.bytes_out} \\(\\~%{integer:cassandra.percent_of_orig}% of original\\) in %{notSpace:cassandra.duration_ms}ms = %{number:cassandra.speed_mb}MB/s.\\s+%{notSpace:cassandra.pkeys_in} total partitions merged to %{notSpace:cassandra.pkeys_out}\\.\\s+Partition merge counts were %{data:cassandra.merge_cnt}","%{_prefix} G.* %{integer:duration:scale(1000000)}ms. %{data}: %{integer:cassandra.eden.orig_bytes} -> %{integer:cassandra.eden.new_bytes}; %{data}: %{integer:cassandra.oldgen.orig_bytes} -> %{integer:cassandra.oldgen.new_bytes};.*","%{_prefix} %{word:cassandra.pool}\\s*(?>%{integer:cassandra.cache_used}\\s*%{integer:cassandra.cache_size}\\s*all|%{integer:cassandra.threads.active}\\s*%{integer:cassandra.threads.pending}\\s*%{integer:cassandra.threads.completed}\\s*%{integer:cassandra.threads.blocked}\\s*%{integer:cassandra.threads.all_time_blocked}|%{integer:cassandra.threads.active}\\s*%{integer:cassanadra.threads.pending})","%{_prefix} %{integer:db.operations} operations were slow in the last %{integer:elapsed_time:scale(1000000)} msecs:\\n%{data}","%{_prefix} %{data:msg}"],
aliases: {
"cassandra_compaction_key": "%{_prefix} %{regex(\"Compacting\"):db.operation}.* %{_keyspace}\\/%{_table}:%{data:partition_key} \\(%{_bytes} bytes\\)",
"cassandra_pool_cleaner": "%{_prefix} %{regex(\"Flushing\"):db.operation}.*\\(Keyspace='%{_keyspace}', ColumnFamily='%{_table}'\\) %{data}: %{_onheap_total}\\/%{_offheap_total}, live: %{_onheap_live}\\/%{_offheap_live}, flushing: %{_onheap_flush}\\/%{_offheap_flush}, this: %{_onheap_this}\\/%{_offheap_this}",
"cassandra_pool_cleaner2": "%{_prefix} %{regex(\"Enqueuing\"):db.operation}.* of %{_keyspace}: %{_onheap_bytes}%{data} \\(%{_onheap_pct}%\\) on-heap, %{_offheap_bytes} \\(%{_offheap_pct}%\\).*",
"cassandra_table_flush": "%{_prefix} %{regex(\"Writing\"):db.operation}.*-%{_keyspace}%{data}\\(%{number:cassandra.bytes:scale(1000000)}%{data}, %{integer:cassandra.ops} ops, %{_onheap_pct}%\\/%{_offheap_pct}.*",
"cassandra_mem_flush": "%{_prefix} Completed %{regex(\"flushing\"):db.operation} %{_sstable} \\(%{number:cassandra.bytes_kb}KiB\\) for commitlog %{data:commitlog}",
"cassandra_compaction": "%{_prefix}\\s+%{regex(\"Compacted\"):db.operation}.* to \\[%{_sstable}\\].\\s+%{notSpace:cassandra.bytes_in} bytes to %{notSpace:cassandra.bytes_out} \\(\\~%{integer:cassandra.percent_of_orig}% of original\\) in %{notSpace:cassandra.duration_ms}ms = %{number:cassandra.speed_mb}MB/s.\\s+%{notSpace:cassandra.pkeys_in} total partitions merged to %{notSpace:cassandra.pkeys_out}\\.\\s+Partition merge counts were %{data:cassandra.merge_cnt}",
"cassandra_gc_format": "%{_prefix} G.* %{integer:duration:scale(1000000)}ms. %{data}: %{integer:cassandra.eden.orig_bytes} -> %{integer:cassandra.eden.new_bytes}; %{data}: %{integer:cassandra.oldgen.orig_bytes} -> %{integer:cassandra.oldgen.new_bytes};.*",
"cassandra_thread_pending": "%{_prefix} %{word:cassandra.pool}\\s*(?>%{integer:cassandra.cache_used}\\s*%{integer:cassandra.cache_size}\\s*all|%{integer:cassandra.threads.active}\\s*%{integer:cassandra.threads.pending}\\s*%{integer:cassandra.threads.completed}\\s*%{integer:cassandra.threads.blocked}\\s*%{integer:cassandra.threads.all_time_blocked}|%{integer:cassandra.threads.active}\\s*%{integer:cassanadra.threads.pending})",
"cassandra_slow_statements": "%{_prefix} %{integer:db.operations} operations were slow in the last %{integer:elapsed_time:scale(1000000)} msecs:\\n%{data}",
"cassandra_fallback_parser": "%{_prefix} %{data:msg}",
"_level": "%{word:db.severity}",
"_thread_name": "%{notSpace:logger.thread_name}",
"_thread_id": "%{integer:logger.thread_id}",
"_logger_name": "%{notSpace:logger.name}",
"_table": "%{word:db.table}",
"_sstable": "%{notSpace:cassandra.sstable}",
"_bytes": "%{integer:cassandra.bytes}",
"_keyspace": "%{word:cassandra.keyspace}",
"_onheap_total": "%{number:cassandra.onheap.total}",
"_onheap_live": "%{number:cassandra.onheap.live}",
"_onheap_flush": "%{number:cassandra.onheap.flush}",
"_onheap_this": "%{number:cassandra.onheap.this}",
"_onheap_bytes": "%{integer:cassandra.onheap.bytes}",
"_onheap_pct": "%{integer:cassandra.onheap.percent}",
"_offheap_total": "%{number:cassandra.offheap.total}",
"_offheap_live": "%{number:cassandra.offheap.live}",
"_offheap_flush": "%{number:cassandra.offheap.flush}",
"_offheap_this": "%{number:cassandra.offheap.this}",
"_offheap_bytes": "%{integer:cassandra.offheap.bytes}",
"_offheap_pct": "%{integer:cassandra.offheap.percent}",
"_default_prefix": "%{_level}\\s+\\[(%{_thread_name}:%{_thread_id}|%{_thread_name})\\]\\s+%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):db.date}\\s+%{word:filename}.java:%{integer:lineno} -",
"_suggested_prefix": "%{date(\"yyyy-MM-dd HH:mm:ss\"):db.date} \\[(%{_thread_name}:%{_thread_id}|%{_thread_name})\\] %{_level} %{_logger_name}\\s+-",
"_prefix": "(?>%{_default_prefix}|%{_suggested_prefix})"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.cassandra.transforms]]
type = "remap"
source = '''
if exists(.custom.db.date) {
.timestamp = .custom.db.date
}
'''

[[transforms.processing.logs.pipelines.cassandra.transforms]]
type = "remap"
source = '''
status = string(.custom.db.severity) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[transforms.processing.logs.pipelines.apache_httpd]
name = "apache_httpd"
filter.type = "datadog_search"
filter.source = "source:httpd"

[[transforms.processing.logs.pipelines.apache_httpd.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)","%{access.common} \"%{_referer}\" \"%{_user_agent}\""],
aliases: {
"access.common": "%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)",
"access.combined": "%{access.common} \"%{_referer}\" \"%{_user_agent}\"",
"_auth": "%{notSpace:http.auth:nullIf(\"-\")}",
"_bytes_written": "%{integer:network.bytes_written}",
"_client_ip": "%{ipOrHost:network.client.ip}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
"_url": "%{notSpace:http.url}",
"_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
"_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
"_referer": "%{notSpace:http.referer}",
"_status_code": "%{integer:http.status_code}",
"_method": "%{word:http.method}",
"_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.apache_httpd.transforms]]
type = "remap"
source = '''
if (details, err = parse_user_agent(.custom.http.useragent); err == null) {
  .custom.http.useragent_details = details
}
'''

[[transforms.processing.logs.pipelines.apache_httpd.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.http.url); err == null) {
  .custom.http.url_details = details
}
'''

[[transforms.processing.logs.pipelines.apache_httpd.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
.timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.pipelines.apache_httpd.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@http.status_code:[200 TO 299]") {
.custom.http.status_category = "OK"
} else if match_datadog_query(., "@http.status_code:[300 TO 399]") {
.custom.http.status_category = "notice"
} else if match_datadog_query(., "@http.status_code:[400 TO 499]") {
.custom.http.status_category = "warning"
} else if match_datadog_query(., "@http.status_code:[500 TO 599]") {
.custom.http.status_category = "error"
}
'''

[[transforms.processing.logs.pipelines.apache_httpd.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[transforms.processing.logs.pipelines.azure_recovery_services]
name = "azure_recovery_services"
filter.type = "datadog_search"
filter.source = "source:azure.recoveryservices"

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
if exists(.custom.category) {
.custom.evt.category = .custom.category
}
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
if exists(.custom.operationName) {
.custom.evt.name = .custom.operationName
}
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
if exists(.custom.resultType) {
.custom.evt.outcome = .custom.resultType
}
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
if exists(.custom.callerIpAddress) {
.custom.network.client.ip = .custom.callerIpAddress
}
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
if (result, err = .custom.durationMs * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
if exists(.custom.identity.authorization.evidence.principalId) {
.custom.usr.id = .custom.identity.authorization.evidence.principalId
}
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
if exists(.custom.properties.Entity_Name) {
.custom.entity_name = .custom.properties.Entity_Name
}
'''

[transforms.processing.logs.pipelines.c_]
name = "c_"
filter.type = "datadog_search"
filter.source = "source:csharp"

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.logger) {
.custom.logger.name = .custom.logger
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.thread) {
.custom.logger.thread_name = .custom.thread
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.exception) {
.custom.error.stack = .custom.exception
} else if exists(.custom.stack_trace) {
.custom.error.stack = .custom.stack_trace
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message,
patterns: ["%{_date} %{_status} \\[%{_thread_name}\\] %{_logger_name} %{_method}:%{_line} - %{data:message}((\\n|\\t)%{data:error.stack})?","(%{_date}|%{_date_ms}) %{_status}\\s+%{data:message}((\\n|\\t)%{data:error.stack})?","%{date(\"yyyy-MM-dd HH:mm:ss.SSS Z\"):date} \\[%{_status}\\] %{data}","(%{date(\"yyyy-MM-dd HH:mm:ss.SSSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSSSS\"):date})\\|%{_status}\\|%{notSpace:logger.name}\\|\\{%{data}\\}.*","%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):date} \\[%{number}\\] %{_status}\\s+%{notSpace:logger.name} \\[%{notSpace}\\] \\{%{data}\\}.*"],
aliases: {
"recommended_format": "%{_date} %{_status} \\[%{_thread_name}\\] %{_logger_name} %{_method}:%{_line} - %{data:message}((\\n|\\t)%{data:error.stack})?",
"default_parser": "(%{_date}|%{_date_ms}) %{_status}\\s+%{data:message}((\\n|\\t)%{data:error.stack})?",
"serilog_format": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS Z\"):date} \\[%{_status}\\] %{data}",
"Nlog_format": "(%{date(\"yyyy-MM-dd HH:mm:ss.SSSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSSSS\"):date})\\|%{_status}\\|%{notSpace:logger.name}\\|\\{%{data}\\}.*",
"log4net_format": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):date} \\[%{number}\\] %{_status}\\s+%{notSpace:logger.name} \\[%{notSpace}\\] \\{%{data}\\}.*",
"_date": "(%{date(\"yyyy-MM-dd HH:mm:ss.SSSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date})",
"_date_ms": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):date}",
"_status": "%{word:level}",
"_thread_name": "%{notSpace:logger.thread_name}",
"_logger_name": "%{notSpace:logger.name}",
"_line": "%{integer:line}",
"_method": "%{notSpace:method}"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
.timestamp = .custom.date
} else if exists(.custom.time) {
.timestamp = .custom.time
} else if exists(.custom.@t) {
.timestamp = .custom.@t
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? string(.custom.Level) ?? string(.custom.@l) ?? ""
if status == "" {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
    .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
    .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
    .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
    .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
    .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
    .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
    .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
    .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
    .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .error.stack,
patterns: ["%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"],
aliases: {
"error_rule": "%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"

})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.trace_id) {
.trace_id = .custom.dd.trace_id
} else if exists(.custom.dd_trace_id) {
.trace_id = .custom.dd_trace_id
} else if exists(.custom.Properties.dd.trace_id) {
.trace_id = .custom.Properties.dd.trace_id
} else if exists(.custom.properties.dd.trace_id) {
.trace_id = .custom.properties.dd.trace_id
} else if exists(.custom.Properties.dd_trace_id) {
.trace_id = .custom.Properties.dd_trace_id
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.RenderedMessage) {
.message = .custom.RenderedMessage
} else if exists(.custom.@m) {
.message = .custom.@m
} else if exists(.custom.@mt) {
.message = .custom.@mt
} else if exists(.custom.message) {
.message = .custom.message
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.env) {
.custom.env = .custom.dd.env
} else if exists(.custom.dd_env) {
.custom.env = .custom.dd_env
} else if exists(.custom.Properties.dd.env) {
.custom.env = .custom.Properties.dd.env
} else if exists(.custom.properties.dd.env) {
.custom.env = .custom.properties.dd.env
} else if exists(.custom.Properties.dd_env) {
.custom.env = .custom.Properties.dd_env
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.version) {
.custom.version = .custom.dd.version
} else if exists(.custom.dd_version) {
.custom.version = .custom.dd_version
} else if exists(.custom.Properties.dd.version) {
.custom.version = .custom.Properties.dd.version
} else if exists(.custom.properties.dd.version) {
.custom.version = .custom.properties.dd.version
} else if exists(.custom.Properties.dd_version) {
.custom.version = .custom.Properties.dd_version
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
.service = .custom.dd.service
} else if exists(.custom.dd_service) {
.service = .custom.dd_service
} else if exists(.custom.Properties.dd.service) {
.service = .custom.Properties.dd.service
} else if exists(.custom.properties.dd.service) {
.service = .custom.properties.dd.service
} else if exists(.custom.Properties.dd_service) {
.service = .custom.Properties.dd_service
}
'''

[transforms.processing.logs.pipelines.web_browser_logs]
name = "web_browser_logs"
filter.type = "datadog_search"
filter.source = "source:browser"

[[transforms.processing.logs.pipelines.web_browser_logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.http.url); err == null) {
  .custom.http.url_details = details
}
'''

[[transforms.processing.logs.pipelines.web_browser_logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_user_agent(.custom.http.useragent); err == null) {
  .custom.http.useragent_details = details
}
'''

[[transforms.processing.logs.pipelines.web_browser_logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.view.url); err == null) {
  .custom.view.url_details = details
}
'''

[[transforms.processing.logs.pipelines.web_browser_logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.view.referrer); err == null) {
  .custom.view.referrer_details = details
}
'''

[[transforms.processing.logs.pipelines.web_browser_logs.transforms]]
type = "remap"
source = '''
#TODO: geo-ip-parser
'''

##
## Sinks
##

[sinks.prometheus]
type = "prometheus_exporter"
inputs = ["internal_metrics"]
address = "0.0.0.0:9090"

[sinks.blackhole]
type = "blackhole"
inputs = ["processing"]
